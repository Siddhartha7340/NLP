{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14589831,"sourceType":"datasetVersion","datasetId":9319357}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers jiwer accelerate soundfile librosa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T11:19:53.960484Z","iopub.execute_input":"2026-01-25T11:19:53.960996Z","iopub.status.idle":"2026-01-25T11:19:59.985607Z","shell.execute_reply.started":"2026-01-25T11:19:53.960966Z","shell.execute_reply":"2026-01-25T11:19:59.984829Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nSpeech-to-Text Model Comparison Script\nEvaluates multiple STT models on accuracy, speed, and resource usage\nUses LibriSpeech dataset from Kaggle\n\nRequired installations:\n!pip install transformers jiwer accelerate soundfile librosa\n\"\"\"\n\nimport time\nimport torch\nimport numpy as np\nimport jiwer\nfrom transformers import (\n    AutoModelForSpeechSeq2Seq, \n    AutoProcessor,\n    pipeline\n)\nimport gc\nimport pandas as pd\nfrom typing import Dict, List\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Ensure audio libraries are available\ntry:\n    import soundfile as sf\n    import librosa\nexcept ImportError:\n    print(\"Installing required audio libraries...\")\n    import subprocess\n    subprocess.check_call(['pip', 'install', '-q', 'soundfile', 'librosa'])\n    import soundfile as sf\n    import librosa\n\n# Configuration\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n\nclass STTModelEvaluator:\n    \"\"\"Evaluate Speech-to-Text models\"\"\"\n    \n    def __init__(self):\n        self.results = []\n        self.device = DEVICE\n        self.torch_dtype = TORCH_DTYPE\n        \n    def load_librispeech_dataset(self, base_path, max_samples=None):\n        \"\"\"Load LibriSpeech dataset from folder structure\n        \n        Args:\n            base_path: Path to LibriSpeech folder (e.g., '/kaggle/input/benchmark/LibriSpeech/test-clean')\n            max_samples: Maximum number of samples to load (None = load all)\n        \n        Expected structure:\n            base_path/\n                speaker_id/\n                    chapter_id/\n                        speaker_id-chapter_id-utterance_id.flac\n                        speaker_id-chapter_id.trans.txt\n        \"\"\"\n        print(f\"Loading LibriSpeech dataset from: {base_path}\")\n        \n        import os\n        from pathlib import Path\n        \n        samples = []\n        transcripts = {}\n        \n        # Walk through the directory structure\n        base_path = Path(base_path)\n        \n        # Find all .trans.txt files\n        trans_files = list(base_path.rglob(\"*.trans.txt\"))\n        print(f\"Found {len(trans_files)} transcript files\")\n        \n        # Parse transcript files\n        for trans_file in trans_files:\n            with open(trans_file, 'r', encoding='utf-8') as f:\n                for line in f:\n                    parts = line.strip().split(' ', 1)\n                    if len(parts) == 2:\n                        utterance_id, text = parts\n                        transcripts[utterance_id] = text.strip()\n        \n        print(f\"Loaded {len(transcripts)} transcripts\")\n        \n        # Find all .flac audio files\n        audio_files = list(base_path.rglob(\"*.flac\"))\n        print(f\"Found {len(audio_files)} audio files\")\n        \n        # Load audio files with their transcripts\n        loaded_count = 0\n        for audio_file in audio_files:\n            # Extract utterance ID from filename\n            utterance_id = audio_file.stem  # e.g., \"1089-134691-0000\"\n            \n            if utterance_id in transcripts:\n                try:\n                    # Load audio\n                    audio, sr = librosa.load(str(audio_file), sr=16000)\n                    \n                    samples.append({\n                        'audio': audio,\n                        'sampling_rate': sr,\n                        'reference': transcripts[utterance_id].upper(),\n                        'file': str(audio_file.name)\n                    })\n                    \n                    loaded_count += 1\n                    print(f\"  Loaded {loaded_count} samples...\", end='\\r')\n                    \n                    # Check if we've reached max_samples\n                    if max_samples and loaded_count >= max_samples:\n                        break\n                        \n                except Exception as e:\n                    print(f\"\\n  Error loading {audio_file}: {e}\")\n                    continue\n        \n        print(f\"\\nâœ“ Successfully loaded {len(samples)} audio samples\")\n        \n        if len(samples) == 0:\n            print(\"âŒ No samples loaded. Please check the dataset path.\")\n            print(f\"   Looking in: {base_path}\")\n            print(f\"   Expected structure: speaker_id/chapter_id/*.flac\")\n        \n        return samples\n    \n    def load_multiple_speakers(self, base_path, speaker_folders, max_samples_per_speaker=None):\n        \"\"\"Load samples from multiple speaker folders\n        \n        Args:\n            base_path: Base path to LibriSpeech (e.g., '/kaggle/input/benchmark/LibriSpeech/test-clean')\n            speaker_folders: List of speaker/chapter paths (e.g., ['1089/134691', '1089/134686'])\n            max_samples_per_speaker: Max samples per speaker folder\n        \"\"\"\n        print(f\"Loading samples from {len(speaker_folders)} speaker folders...\")\n        \n        all_samples = []\n        \n        for speaker_folder in speaker_folders:\n            folder_path = f\"{base_path}/{speaker_folder}\"\n            print(f\"\\nLoading from: {folder_path}\")\n            \n            samples = self.load_librispeech_dataset(folder_path, max_samples_per_speaker)\n            all_samples.extend(samples)\n        \n        print(f\"\\nâœ“ Total samples loaded: {len(all_samples)}\")\n        return all_samples\n    \n    def evaluate_whisper_large_v3(self, test_data: List[Dict]) -> Dict:\n        \"\"\"Evaluate OpenAI Whisper Large v3\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"Evaluating: Whisper Large v3\")\n        print(\"=\"*60)\n        \n        model_id = \"openai/whisper-large-v3\"\n        \n        try:\n            # Load model\n            start_load = time.time()\n            model = AutoModelForSpeechSeq2Seq.from_pretrained(\n                model_id,\n                torch_dtype=self.torch_dtype,\n                low_cpu_mem_usage=True,\n                use_safetensors=True\n            )\n            model.to(self.device)\n            \n            processor = AutoProcessor.from_pretrained(model_id)\n            \n            pipe = pipeline(\n                \"automatic-speech-recognition\",\n                model=model,\n                tokenizer=processor.tokenizer,\n                feature_extractor=processor.feature_extractor,\n                max_new_tokens=128,\n                chunk_length_s=30,\n                batch_size=16,\n                return_timestamps=False,\n                torch_dtype=self.torch_dtype,\n                device=self.device,\n            )\n            load_time = time.time() - start_load\n            print(f\"âœ“ Model loaded in {load_time:.2f}s\")\n            \n            # Run inference\n            predictions = []\n            references = []\n            inference_times = []\n            \n            for idx, sample in enumerate(test_data):\n                start = time.time()\n                result = pipe(sample['audio'])\n                inference_times.append(time.time() - start)\n                \n                pred_text = result['text'].strip().upper()\n                predictions.append(pred_text)\n                references.append(sample['reference'].upper())\n                \n                print(f\"  Sample {idx+1}/{len(test_data)}: {inference_times[-1]:.3f}s\")\n                if idx < 3:  # Show first 3 predictions\n                    print(f\"    Predicted: {pred_text[:60]}...\")\n                    print(f\"    Reference: {sample['reference'][:60]}...\")\n            \n            # Calculate metrics\n            wer = jiwer.wer(references, predictions)\n            avg_inference_time = np.mean(inference_times)\n            \n            print(f\"\\nâœ“ WER: {wer*100:.2f}%\")\n            print(f\"âœ“ Avg inference: {avg_inference_time:.3f}s\")\n            \n            results = {\n                'model': 'Whisper Large v3',\n                'wer': wer * 100,\n                'load_time': load_time,\n                'avg_inference_time': avg_inference_time,\n                'total_inference_time': sum(inference_times),\n                'model_size': 'Large (~1.5GB)',\n                'notes': 'Multilingual, 99+ languages'\n            }\n            \n            # Cleanup\n            del model, processor, pipe\n            gc.collect()\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            \n            return results\n            \n        except Exception as e:\n            print(f\"âŒ Error evaluating Whisper Large v3: {e}\")\n            import traceback\n            traceback.print_exc()\n            return None\n    \n    def evaluate_distil_whisper(self, test_data: List[Dict]) -> Dict:\n        \"\"\"Evaluate Distil-Whisper Large v3\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"Evaluating: Distil-Whisper Large v3\")\n        print(\"=\"*60)\n        \n        model_id = \"distil-whisper/distil-large-v3\"\n        \n        try:\n            start_load = time.time()\n            model = AutoModelForSpeechSeq2Seq.from_pretrained(\n                model_id,\n                torch_dtype=self.torch_dtype,\n                low_cpu_mem_usage=True,\n                use_safetensors=True\n            )\n            model.to(self.device)\n            \n            processor = AutoProcessor.from_pretrained(model_id)\n            \n            pipe = pipeline(\n                \"automatic-speech-recognition\",\n                model=model,\n                tokenizer=processor.tokenizer,\n                feature_extractor=processor.feature_extractor,\n                max_new_tokens=128,\n                chunk_length_s=30,\n                batch_size=16,\n                torch_dtype=self.torch_dtype,\n                device=self.device,\n            )\n            load_time = time.time() - start_load\n            print(f\"âœ“ Model loaded in {load_time:.2f}s\")\n            \n            predictions = []\n            references = []\n            inference_times = []\n            \n            for idx, sample in enumerate(test_data):\n                start = time.time()\n                result = pipe(sample['audio'])\n                inference_times.append(time.time() - start)\n                \n                pred_text = result['text'].strip().upper()\n                predictions.append(pred_text)\n                references.append(sample['reference'].upper())\n                \n                print(f\"  Sample {idx+1}/{len(test_data)}: {inference_times[-1]:.3f}s\")\n                if idx < 3:\n                    print(f\"    Predicted: {pred_text[:60]}...\")\n                    print(f\"    Reference: {sample['reference'][:60]}...\")\n            \n            wer = jiwer.wer(references, predictions)\n            avg_inference_time = np.mean(inference_times)\n            \n            print(f\"\\nâœ“ WER: {wer*100:.2f}%\")\n            print(f\"âœ“ Avg inference: {avg_inference_time:.3f}s\")\n            \n            results = {\n                'model': 'Distil-Whisper Large v3',\n                'wer': wer * 100,\n                'load_time': load_time,\n                'avg_inference_time': avg_inference_time,\n                'total_inference_time': sum(inference_times),\n                'model_size': 'Medium (~756MB)',\n                'notes': '6x faster than Whisper, minimal accuracy loss'\n            }\n            \n            del model, processor, pipe\n            gc.collect()\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            \n            return results\n            \n        except Exception as e:\n            print(f\"âŒ Error evaluating Distil-Whisper: {e}\")\n            import traceback\n            traceback.print_exc()\n            return None\n    \n    def evaluate_whisper_base(self, test_data: List[Dict]) -> Dict:\n        \"\"\"Evaluate Whisper Base (faster, smaller)\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"Evaluating: Whisper Base\")\n        print(\"=\"*60)\n        \n        model_id = \"openai/whisper-base\"\n        \n        try:\n            start_load = time.time()\n            model = AutoModelForSpeechSeq2Seq.from_pretrained(\n                model_id,\n                torch_dtype=self.torch_dtype,\n                low_cpu_mem_usage=True,\n                use_safetensors=True\n            )\n            model.to(self.device)\n            \n            processor = AutoProcessor.from_pretrained(model_id)\n            \n            pipe = pipeline(\n                \"automatic-speech-recognition\",\n                model=model,\n                tokenizer=processor.tokenizer,\n                feature_extractor=processor.feature_extractor,\n                max_new_tokens=128,\n                chunk_length_s=30,\n                batch_size=16,\n                torch_dtype=self.torch_dtype,\n                device=self.device,\n            )\n            load_time = time.time() - start_load\n            print(f\"âœ“ Model loaded in {load_time:.2f}s\")\n            \n            predictions = []\n            references = []\n            inference_times = []\n            \n            for idx, sample in enumerate(test_data):\n                start = time.time()\n                result = pipe(sample['audio'])\n                inference_times.append(time.time() - start)\n                \n                pred_text = result['text'].strip().upper()\n                predictions.append(pred_text)\n                references.append(sample['reference'].upper())\n                \n                print(f\"  Sample {idx+1}/{len(test_data)}: {inference_times[-1]:.3f}s\")\n                if idx < 3:\n                    print(f\"    Predicted: {pred_text[:60]}...\")\n                    print(f\"    Reference: {sample['reference'][:60]}...\")\n            \n            wer = jiwer.wer(references, predictions)\n            avg_inference_time = np.mean(inference_times)\n            \n            print(f\"\\nâœ“ WER: {wer*100:.2f}%\")\n            print(f\"âœ“ Avg inference: {avg_inference_time:.3f}s\")\n            \n            results = {\n                'model': 'Whisper Base',\n                'wer': wer * 100,\n                'load_time': load_time,\n                'avg_inference_time': avg_inference_time,\n                'total_inference_time': sum(inference_times),\n                'model_size': 'Small (~140MB)',\n                'notes': 'Fast inference, good for real-time'\n            }\n            \n            del model, processor, pipe\n            gc.collect()\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            \n            return results\n            \n        except Exception as e:\n            print(f\"âŒ Error evaluating Whisper Base: {e}\")\n            import traceback\n            traceback.print_exc()\n            return None\n    \n    def run_comparison(self, test_data, num_samples=10):\n        \"\"\"Run complete comparison\"\"\"\n        print(\"=\"*80)\n        print(\"SPEECH-TO-TEXT MODEL COMPARISON\")\n        print(\"=\"*80)\n        print(f\"Device: {self.device}\")\n        print(f\"Torch dtype: {self.torch_dtype}\")\n        \n        if len(test_data) == 0:\n            print(\"âŒ No test data available. Please provide audio samples.\")\n            return None\n        \n        print(f\"\\nRunning tests on {len(test_data)} samples...\")\n        \n        # Evaluate models\n        results = []\n        \n        # Whisper Base (fastest)\n        result = self.evaluate_whisper_base(test_data)\n        if result:\n            results.append(result)\n        \n        # Distil-Whisper (balanced)\n        result = self.evaluate_distil_whisper(test_data)\n        if result:\n            results.append(result)\n        \n        # Whisper Large v3 (most accurate)\n        result = self.evaluate_whisper_large_v3(test_data)\n        if result:\n            results.append(result)\n        \n        # Create results dataframe\n        if results:\n            df = pd.DataFrame(results)\n            return df\n        else:\n            print(\"âŒ No models completed successfully\")\n            return None\n    \n    def display_results(self, df: pd.DataFrame):\n        \"\"\"Display comparison results\"\"\"\n        if df is None or len(df) == 0:\n            print(\"No results to display\")\n            return\n            \n        print(\"\\n\" + \"=\"*80)\n        print(\"RESULTS SUMMARY\")\n        print(\"=\"*80)\n        print(f\"\\nMetrics:\")\n        print(df.to_string(index=False))\n        \n        print(\"\\n\" + \"-\"*80)\n        print(\"KEY METRICS:\")\n        print(\"-\"*80)\n        print(\"WER (Word Error Rate): Lower is better (% of words incorrectly transcribed)\")\n        print(\"Avg Inference Time: Time per audio sample (seconds)\")\n        print(\"Load Time: Model loading time (seconds)\")\n        \n        print(\"\\n\" + \"-\"*80)\n        print(\"RECOMMENDATIONS:\")\n        print(\"-\"*80)\n        \n        # Find best models\n        best_accuracy = df.loc[df['wer'].idxmin()]\n        best_speed = df.loc[df['avg_inference_time'].idxmin()]\n        \n        print(f\"\\nðŸŽ¯ Best Accuracy: {best_accuracy['model']} (WER: {best_accuracy['wer']:.2f}%)\")\n        print(f\"âš¡ Fastest: {best_speed['model']} ({best_speed['avg_inference_time']:.3f}s per sample)\")\n        \n        print(\"\\nðŸ’¡ Use Case Recommendations:\")\n        print(\"   â€¢ Real-time applications â†’ Whisper Base\")\n        print(\"   â€¢ Balanced performance â†’ Distil-Whisper Large v3\")\n        print(\"   â€¢ Maximum accuracy â†’ Whisper Large v3\")\n        print(\"   â€¢ Production/API â†’ Deepgram Nova-2 (requires API key)\")\n        \n        # Save results\n        df.to_csv('stt_comparison_results.csv', index=False)\n        print(\"\\nâœ… Results saved to: stt_comparison_results.csv\")\n\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"\"\"\n    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n    â•‘         SPEECH-TO-TEXT MODEL COMPARISON TOOL                   â•‘\n    â•‘                                                                â•‘\n    â•‘  This script compares multiple STT models on:                  â•‘\n    â•‘  â€¢ Accuracy (Word Error Rate)                                  â•‘\n    â•‘  â€¢ Speed (Inference Time)                                      â•‘\n    â•‘  â€¢ Model Size                                                  â•‘\n    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    \"\"\")\n    \n    evaluator = STTModelEvaluator()\n    \n    # Load LibriSpeech dataset from multiple speaker folders\n    test_data = evaluator.load_multiple_speakers(\n        base_path='/kaggle/input/benchmark/LibriSpeech/test-clean',\n        speaker_folders=['1089/134691', '1089/134686'],\n        max_samples_per_speaker=10  # 10 samples from each folder = 20 total\n    )\n    \n    # Run comparison\n    results_df = evaluator.run_comparison(test_data)\n    \n    # Display results\n    evaluator.display_results(results_df)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"âœ… Comparison complete!\")\n    print(\"=\"*80)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-23T06:59:46.259168Z","iopub.execute_input":"2026-01-23T06:59:46.260020Z","iopub.status.idle":"2026-01-23T07:02:16.126550Z","shell.execute_reply.started":"2026-01-23T06:59:46.259985Z","shell.execute_reply":"2026-01-23T07:02:16.125833Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"2026-01-23 06:59:56.582896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769151596.775146      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769151596.833781      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769151597.395993      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769151597.396040      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769151597.396043      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769151597.396046      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"\n    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n    â•‘         SPEECH-TO-TEXT MODEL COMPARISON TOOL                   â•‘\n    â•‘                                                                â•‘\n    â•‘  This script compares multiple STT models on:                  â•‘\n    â•‘  â€¢ Accuracy (Word Error Rate)                                  â•‘\n    â•‘  â€¢ Speed (Inference Time)                                      â•‘\n    â•‘  â€¢ Model Size                                                  â•‘\n    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    \nLoading samples from 2 speaker folders...\n\nLoading from: /kaggle/input/benchmark/LibriSpeech/test-clean/1089/134691\nLoading LibriSpeech dataset from: /kaggle/input/benchmark/LibriSpeech/test-clean/1089/134691\nFound 1 transcript files\nLoaded 26 transcripts\nFound 26 audio files\n  Loaded 10 samples...\nâœ“ Successfully loaded 10 audio samples\n\nLoading from: /kaggle/input/benchmark/LibriSpeech/test-clean/1089/134686\nLoading LibriSpeech dataset from: /kaggle/input/benchmark/LibriSpeech/test-clean/1089/134686\nFound 1 transcript files\nLoaded 38 transcripts\nFound 38 audio files\n  Loaded 10 samples...\nâœ“ Successfully loaded 10 audio samples\n\nâœ“ Total samples loaded: 20\n================================================================================\nSPEECH-TO-TEXT MODEL COMPARISON\n================================================================================\nDevice: cuda\nTorch dtype: torch.float16\n\nRunning tests on 20 samples...\n\n============================================================\nEvaluating: Whisper Base\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"749571a6d6964df79876b5e0fa9ae7aa"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee555df978744a6986d1e346c70c7720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2d1ea25846341b1bee9727962806225"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"133749df703940b09fe790b9430a21fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ed15055fe0a44eba592c89ab9ee222e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d359924e9f44198294bb82232f3a10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab07ade9689409088424b6c7f6028a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b859d77c4c9c440998097dbc71e48fb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f83698e31d04d9b995b20c658549c65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab001bb668444290bf2141e11044e499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee68a674964b4585b14177b689c751fe"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\nDevice set to use cuda\nUsing `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\nUsing custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\nTranscription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ Model loaded in 11.45s\n  Sample 1/20: 2.028s\n    Predicted: IDLE AND IMBITERING, FINALLY, TO ARGUE AGAINST HIS OWN DISPA...\n    Reference: IDLE AND EMBITTERING FINALLY TO ARGUE AGAINST HIS OWN DISPAS...\n  Sample 2/20: 0.286s\n    Predicted: THE PRIDE OF THAT DIM IMAGE BROUGHT BACK TO HIS MIND THE DIG...\n    Reference: THE PRIDE OF THAT DIM IMAGE BROUGHT BACK TO HIS MIND THE DIG...\n  Sample 3/20: 0.292s\n    Predicted: FOR A FULL HOUR HE HAD PACED UP AND DOWN, WAITING, BUT HE CO...\n    Reference: FOR A FULL HOUR HE HAD PACED UP AND DOWN WAITING BUT HE COUL...\n  Sample 4/20: 0.240s\n  Sample 5/20: 0.219s\n  Sample 6/20: 0.237s\n  Sample 7/20: 0.166s\n  Sample 8/20: 0.576s\n  Sample 9/20: 0.146s\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"  Sample 10/20: 0.116s\n  Sample 11/20: 0.260s\n  Sample 12/20: 0.377s\n  Sample 13/20: 0.299s\n  Sample 14/20: 0.257s\n  Sample 15/20: 0.293s\n  Sample 16/20: 0.236s\n  Sample 17/20: 0.279s\n  Sample 18/20: 0.427s\n  Sample 19/20: 0.278s\n  Sample 20/20: 0.149s\n\nâœ“ WER: 21.29%\nâœ“ Avg inference: 0.358s\n\n============================================================\nEvaluating: Distil-Whisper Large v3\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a9da9bac8f148b08a63f350b4943e23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.51G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa9dec861be840a99ce1d80fb0146e7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de7d92d35a33442fa04e219e182170f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc89b90a106433abaec287d2bc92cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"695cd73046184289a8c7418ee0d7dfc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510d5b9e63904630bf10e8a4de3c90fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01f5c9c1b2054aa1aa4db1e7a9211098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efccc67ba67d4615a3e5c6d712f5d9ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4e28571c8745929b5abe2ba90947fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa84e08566ea47f3bfaae3f55ca6d0de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0b86f7d1464dd6b823c5c5ca582132"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda\nUsing `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n","output_type":"stream"},{"name":"stdout","text":"âœ“ Model loaded in 14.23s\n  Sample 1/20: 0.867s\n    Predicted: IDLE AND EMBITTERING FINALLY TO ARGUE AGAINST HIS OWN DISPAS...\n    Reference: IDLE AND EMBITTERING FINALLY TO ARGUE AGAINST HIS OWN DISPAS...\n  Sample 2/20: 0.579s\n    Predicted: THE PRIDE OF THAT DIM IMAGE BROUGHT BACK TO HIS MIND THE DIG...\n    Reference: THE PRIDE OF THAT DIM IMAGE BROUGHT BACK TO HIS MIND THE DIG...\n  Sample 3/20: 0.593s\n    Predicted: FOR A FULL HOUR HE HAD PACED UP AND DOWN, WAITING, BUT HE CO...\n    Reference: FOR A FULL HOUR HE HAD PACED UP AND DOWN WAITING BUT HE COUL...\n  Sample 4/20: 0.528s\n  Sample 5/20: 0.493s\n  Sample 6/20: 0.530s\n  Sample 7/20: 0.484s\n  Sample 8/20: 0.767s\n  Sample 9/20: 0.467s\n  Sample 10/20: 0.444s\n  Sample 11/20: 0.552s\n  Sample 12/20: 0.646s\n  Sample 13/20: 0.583s\n  Sample 14/20: 0.551s\n  Sample 15/20: 0.576s\n  Sample 16/20: 0.535s\n  Sample 17/20: 0.567s\n  Sample 18/20: 0.681s\n  Sample 19/20: 0.569s\n  Sample 20/20: 0.467s\n\nâœ“ WER: 13.23%\nâœ“ Avg inference: 0.574s\n\n============================================================\nEvaluating: Whisper Large v3\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b950119f828644e6b8986c1299760081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27cfa05933447ba90327ea8f18e783a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b4d28ed50245eb8eaf5b612f5e97b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c96a50e688940b0bd6a38254fa8a215"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267f168c0de74cb6ad8b62c7c969b05e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5975c72fac9f44a28e19fe034cdc3dd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69ad1770ac124911b5f6adeb9632b4df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e4fcca99ae743da92b67990045cae0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"202f4c929ecc4c43bb66e8b0a6ee1f1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd69d20d348b4a19a6d1a9475f38a3c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d20dd8b274d4443fa4083690b65b72ac"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda\nUsing `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n","output_type":"stream"},{"name":"stdout","text":"âœ“ Model loaded in 16.68s\n  Sample 1/20: 4.614s\n    Predicted: IDLE AND EMBITTERING FINALLY TO ARGUE AGAINST HIS OWN DISPAS...\n    Reference: IDLE AND EMBITTERING FINALLY TO ARGUE AGAINST HIS OWN DISPAS...\n  Sample 2/20: 2.421s\n    Predicted: THE PRIDE OF THAT DIM IMAGE BROUGHT BACK TO HIS MIND THE DIG...\n    Reference: THE PRIDE OF THAT DIM IMAGE BROUGHT BACK TO HIS MIND THE DIG...\n  Sample 3/20: 2.607s\n    Predicted: FOR A FULL HOUR HE HAD PACED UP AND DOWN, WAITING, BUT HE CO...\n    Reference: FOR A FULL HOUR HE HAD PACED UP AND DOWN WAITING BUT HE COUL...\n  Sample 4/20: 2.354s\n  Sample 5/20: 1.843s\n  Sample 6/20: 2.157s\n  Sample 7/20: 1.715s\n  Sample 8/20: 4.016s\n  Sample 9/20: 1.782s\n  Sample 10/20: 1.397s\n  Sample 11/20: 2.287s\n  Sample 12/20: 2.988s\n  Sample 13/20: 2.744s\n  Sample 14/20: 2.290s\n  Sample 15/20: 2.416s\n  Sample 16/20: 2.224s\n  Sample 17/20: 2.416s\n  Sample 18/20: 3.373s\n  Sample 19/20: 2.417s\n  Sample 20/20: 1.592s\n\nâœ“ WER: 12.58%\nâœ“ Avg inference: 2.483s\n\n================================================================================\nRESULTS SUMMARY\n================================================================================\n\nMetrics:\n                  model       wer  load_time  avg_inference_time  total_inference_time      model_size                                         notes\n           Whisper Base 21.290323  11.451841            0.358044              7.160876  Small (~140MB)            Fast inference, good for real-time\nDistil-Whisper Large v3 13.225806  14.233587            0.573707             11.474143 Medium (~756MB) 6x faster than Whisper, minimal accuracy loss\n       Whisper Large v3 12.580645  16.680190            2.482687             49.653736  Large (~1.5GB)                   Multilingual, 99+ languages\n\n--------------------------------------------------------------------------------\nKEY METRICS:\n--------------------------------------------------------------------------------\nWER (Word Error Rate): Lower is better (% of words incorrectly transcribed)\nAvg Inference Time: Time per audio sample (seconds)\nLoad Time: Model loading time (seconds)\n\n--------------------------------------------------------------------------------\nRECOMMENDATIONS:\n--------------------------------------------------------------------------------\n\nðŸŽ¯ Best Accuracy: Whisper Large v3 (WER: 12.58%)\nâš¡ Fastest: Whisper Base (0.358s per sample)\n\nðŸ’¡ Use Case Recommendations:\n   â€¢ Real-time applications â†’ Whisper Base\n   â€¢ Balanced performance â†’ Distil-Whisper Large v3\n   â€¢ Maximum accuracy â†’ Whisper Large v3\n   â€¢ Production/API â†’ Deepgram Nova-2 (requires API key)\n\nâœ… Results saved to: stt_comparison_results.csv\n\n================================================================================\nâœ… Comparison complete!\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\"\"\"\nFixed STT benchmark: uses nvidia/canary-1b-flash for Canary (has HF preprocessor)\nRequires: bitsandbytes, accelerate, transformers, jiwer, librosa, soundfile\n\"\"\"\n\nimport time\nimport gc\nimport torch\nimport numpy as np\nimport pandas as pd\nimport jiwer\nimport librosa\nfrom typing import List, Dict\n\nfrom transformers import (\n    pipeline,\n    BitsAndBytesConfig,\n    AutoModelForSpeechSeq2Seq,\n    AutoProcessor,\n)\n\n# -------------------------\n# Config\n# -------------------------\nUSE_CUDA = torch.cuda.is_available()\nDEVICE_IDX = 0 if USE_CUDA else -1  # pipeline expects GPU index or -1 for CPU\nDTYPE = torch.float16 if USE_CUDA else torch.float32\n\n# -------------------------\n# Evaluator\n# -------------------------\nclass STTModelEvaluator:\n    def __init__(self):\n        self.device_idx = DEVICE_IDX\n        self.dtype = DTYPE\n\n    # -------------------------\n    # Data loading\n    # -------------------------\n    def load_librispeech_dataset(self, base_path, max_samples=None):\n        from pathlib import Path\n        samples = []\n        transcripts = {}\n        base_path = Path(base_path)\n\n        for trans_file in base_path.rglob(\"*.trans.txt\"):\n            with open(trans_file, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    uid, text = line.strip().split(\" \", 1)\n                    transcripts[uid] = text.strip()\n\n        count = 0\n        for audio_file in base_path.rglob(\"*.flac\"):\n            uid = audio_file.stem\n            if uid in transcripts:\n                audio, sr = librosa.load(str(audio_file), sr=16000)\n                samples.append({\"audio\": audio, \"reference\": transcripts[uid].upper()})\n                count += 1\n                if max_samples and count >= max_samples:\n                    break\n\n        print(f\"Loaded {len(samples)} samples from {base_path}\")\n        return samples\n\n    def load_multiple_speakers(self, base_path, speaker_folders, max_samples_per_speaker=3):\n        all_samples = []\n        for folder in speaker_folders:\n            all_samples.extend(\n                self.load_librispeech_dataset(f\"{base_path}/{folder}\", max_samples=max_samples_per_speaker)\n            )\n        print(f\"Total samples: {len(all_samples)}\")\n        return all_samples\n\n    # -------------------------\n    # String-based pipeline evaluation (for models that work with simple pipeline)\n    # -------------------------\n    def evaluate_with_pipeline_string(self, model_id: str, name: str, data: List[Dict], size: str, notes: str):\n        print(f\"\\nEvaluating: {name} (pipeline string)\")\n        start_load = time.time()\n\n        pipe = pipeline(\n            \"automatic-speech-recognition\",\n            model=model_id,\n            device=self.device_idx,\n            torch_dtype=self.dtype,\n        )\n\n        load_time = time.time() - start_load\n        preds, refs, times = [], [], []\n\n        for i, sample in enumerate(data):\n            t0 = time.time()\n            out = pipe(sample[\"audio\"])\n            times.append(time.time() - t0)\n            preds.append(out.get(\"text\", \"\").strip().upper())\n            refs.append(sample[\"reference\"].upper())\n            print(f\"  Sample {i+1}/{len(data)}: {times[-1]:.3f}s\")\n\n        wer = jiwer.wer(refs, preds)\n        del pipe\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return {\n            \"model\": name,\n            \"wer\": wer * 100,\n            \"load_time\": load_time,\n            \"avg_inference_time\": float(np.mean(times)),\n            \"total_inference_time\": float(sum(times)),\n            \"model_size\": size,\n            \"notes\": notes,\n        }\n\n    # -------------------------\n    # Manual load for quantized models (e.g., Canary)\n    # -------------------------\n    def evaluate_with_manual_load(self, model_id: str, name: str, data: List[Dict], size: str, notes: str, quantize_8bit: bool = False):\n        \"\"\"\n        Load processor + model via from_pretrained; for 8-bit use BitsAndBytesConfig + device_map='auto'.\n        If processor load fails for the given model_id, attempt fallback to 'nvidia/canary-1b-flash'.\n        \"\"\"\n        print(f\"\\nEvaluating: {name} (manual load, quantize_8bit={quantize_8bit})\")\n\n        # Try to load processor; if it fails, attempt fallback to canary-1b-flash\n        try:\n            processor = AutoProcessor.from_pretrained(model_id)\n        except Exception as e:\n            print(f\"Warning: failed to load processor for {model_id}: {e}\")\n            fallback = \"nvidia/canary-1b-flash\"\n            if model_id != fallback:\n                print(f\"Attempting fallback processor: {fallback}\")\n                try:\n                    processor = AutoProcessor.from_pretrained(fallback)\n                    model_id = fallback\n                    print(f\"Using fallback model_id: {model_id}\")\n                except Exception as e2:\n                    raise RuntimeError(f\"Failed to load processor for both {model_id} and fallback {fallback}: {e2}\")\n            else:\n                raise RuntimeError(f\"Failed to load processor for {model_id}: {e}\")\n\n        # Prepare quantization config if requested\n        quant_config = BitsAndBytesConfig(load_in_8bit=True) if quantize_8bit else None\n\n        start_load = time.time()\n        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n            model_id,\n            quantization_config=quant_config,\n            device_map=\"auto\" if quantize_8bit and torch.cuda.is_available() else None,\n            torch_dtype=self.dtype,\n            low_cpu_mem_usage=True,\n        )\n\n        pipe = pipeline(\n            \"automatic-speech-recognition\",\n            model=model,\n            tokenizer=processor.tokenizer,\n            feature_extractor=processor.feature_extractor,\n            device=self.device_idx,\n            torch_dtype=self.dtype,\n        )\n\n        load_time = time.time() - start_load\n\n        preds, refs, times = [], [], []\n        for i, sample in enumerate(data):\n            t0 = time.time()\n            out = pipe(sample[\"audio\"])\n            times.append(time.time() - t0)\n            preds.append(out.get(\"text\", \"\").strip().upper())\n            refs.append(sample[\"reference\"].upper())\n            print(f\"  Sample {i+1}/{len(data)}: {times[-1]:.3f}s\")\n\n        wer = jiwer.wer(refs, preds)\n\n        # cleanup\n        del pipe, model, processor\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return {\n            \"model\": name,\n            \"wer\": wer * 100,\n            \"load_time\": load_time,\n            \"avg_inference_time\": float(np.mean(times)),\n            \"total_inference_time\": float(sum(times)),\n            \"model_size\": size,\n            \"notes\": notes,\n        }\n\n    # -------------------------\n    # Run full suite\n    # -------------------------\n    def run_all(self, data: List[Dict]):\n        results = []\n\n        results.append(self.evaluate_with_pipeline_string(\n            \"openai/whisper-base\", \"Whisper Base\", data, \"~140MB\", \"Fastest baseline\"\n        ))\n\n        results.append(self.evaluate_with_pipeline_string(\n            \"distil-whisper/distil-large-v3\", \"Distil-Whisper Large v3\", data, \"~756MB\", \"Speed/accuracy tradeoff\"\n        ))\n\n        results.append(self.evaluate_with_pipeline_string(\n            \"openai/whisper-large-v3\", \"Whisper Large v3\", data, \"~1.5GB\", \"High accuracy, slower\"\n        ))\n\n        results.append(self.evaluate_with_pipeline_string(\n            \"facebook/wav2vec2-large-960h-lv60-self\", \"wav2vec2 Large\", data, \"~1.2GB\", \"CTC model; may need fine-tuning\"\n        ))\n\n        # Use Canary Flash variant (has preprocessor_config.json on HF)\n        results.append(self.evaluate_with_manual_load(\n            \"nvidia/canary-1b-flash\", \"Canary 1B Flash\", data, \"~1.2GB\", \"NVIDIA Canary (8-bit load)\", quantize_8bit=True\n        ))\n\n        return pd.DataFrame(results)\n\n\n# -------------------------\n# Main\n# -------------------------\nif __name__ == \"__main__\":\n    evaluator = STTModelEvaluator()\n\n    test_data = evaluator.load_multiple_speakers(\n        base_path=\"/kaggle/input/benchmark/LibriSpeech/test-clean\",\n        speaker_folders=[\"1089/134691\", \"1089/134686\"],\n        max_samples_per_speaker=3\n    )\n\n    df = evaluator.run_all(test_data)\n    print(\"\\n====== RESULTS ======\")\n    print(df.to_string(index=False))\n    df.to_csv(\"stt_results_canary_fixed.csv\", index=False)\n    print(\"\\nSaved: stt_results_canary_fixed.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T11:37:07.454749Z","iopub.execute_input":"2026-01-25T11:37:07.455502Z","iopub.status.idle":"2026-01-25T11:37:40.921000Z","shell.execute_reply.started":"2026-01-25T11:37:07.455468Z","shell.execute_reply":"2026-01-25T11:37:40.919939Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Loaded 3 samples from /kaggle/input/benchmark/LibriSpeech/test-clean/1089/134691\nLoaded 3 samples from /kaggle/input/benchmark/LibriSpeech/test-clean/1089/134686\nTotal samples: 6\n\nEvaluating: Whisper Base (pipeline string)\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"  Sample 1/6: 0.731s\n  Sample 2/6: 0.295s\n  Sample 3/6: 0.299s\n  Sample 4/6: 0.268s\n  Sample 5/6: 0.395s\n  Sample 6/6: 0.307s\n\nEvaluating: Distil-Whisper Large v3 (pipeline string)\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"  Sample 1/6: 0.845s\n  Sample 2/6: 0.580s\n  Sample 3/6: 0.595s\n  Sample 4/6: 0.556s\n  Sample 5/6: 0.653s\n  Sample 6/6: 0.588s\n\nEvaluating: Whisper Large v3 (pipeline string)\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"  Sample 1/6: 4.625s\n  Sample 2/6: 2.426s\n  Sample 3/6: 2.618s\n  Sample 4/6: 2.295s\n  Sample 5/6: 3.007s\n  Sample 6/6: 2.744s\n\nEvaluating: wav2vec2 Large (pipeline string)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"  Sample 1/6: 0.127s\n  Sample 2/6: 0.052s\n  Sample 3/6: 0.050s\n  Sample 4/6: 0.050s\n  Sample 5/6: 0.090s\n  Sample 6/6: 0.052s\n\nEvaluating: Canary 1B Flash (manual load, quantize_8bit=True)\nWarning: failed to load processor for nvidia/canary-1b-flash: The checkpoint you are trying to load has model type `fastconformer` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mclass_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AutoConfig\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"--\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSPECIAL_MODEL_TYPE_TO_MODULE_NAME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPECIAL_MODEL_TYPE_TO_MODULE_NAME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'fastconformer'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2226861388.py\u001b[0m in \u001b[0;36mevaluate_with_manual_load\u001b[0;34m(self, model_id, name, data, size, notes, quantize_8bit)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/processing_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m                     )\n\u001b[0;32m--> 363\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m                 \u001b[0mprocessor_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"processor_class\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"--\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m                 \u001b[0mupstream_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `fastconformer` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2226861388.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n====== RESULTS ======\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2226861388.py\u001b[0m in \u001b[0;36mrun_all\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# Use Canary Flash variant (has preprocessor_config.json on HF)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         results.append(self.evaluate_with_manual_load(\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0;34m\"nvidia/canary-1b-flash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Canary 1B Flash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"~1.2GB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NVIDIA Canary (8-bit load)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         ))\n","\u001b[0;32m/tmp/ipykernel_55/2226861388.py\u001b[0m in \u001b[0;36mevaluate_with_manual_load\u001b[0;34m(self, model_id, name, data, size, notes, quantize_8bit)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load processor for both {model_id} and fallback {fallback}: {e2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load processor for {model_id}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Prepare quantization config if requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to load processor for nvidia/canary-1b-flash: The checkpoint you are trying to load has model type `fastconformer` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"],"ename":"RuntimeError","evalue":"Failed to load processor for nvidia/canary-1b-flash: The checkpoint you are trying to load has model type `fastconformer` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"!pip install -U nemo_toolkit[asr] soundfile librosa jiwer accelerate bitsandbytes transformers\n!apt-get update -y && apt-get install -y libsndfile1 ffmpeg\n!pip install sentence-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T12:45:44.838571Z","iopub.execute_input":"2026-01-25T12:45:44.838908Z","iopub.status.idle":"2026-01-25T12:45:59.392791Z","shell.execute_reply.started":"2026-01-25T12:45:44.838881Z","shell.execute_reply":"2026-01-25T12:45:59.391981Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\nRequirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\nRequirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (3.1.0)\nCollecting jiwer\n  Using cached jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.53.3)\nCollecting transformers\n  Using cached transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\nRequirement already satisfied: nemo_toolkit[asr] in /usr/local/lib/python3.12/dist-packages (2.6.1)\nRequirement already satisfied: fsspec==2024.12.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2024.12.0)\nRequirement already satisfied: huggingface_hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.36.0)\nRequirement already satisfied: numba-cuda==0.15.1 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.15.1)\nRequirement already satisfied: cuda-bindings in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (13.1.1)\nRequirement already satisfied: numexpr<2.14.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.13.1)\nRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.0.2)\nRequirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.20.1)\nRequirement already satisfied: protobuf~=5.29.5 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (5.29.5)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.9.0.post0)\nRequirement already satisfied: ruamel.yaml in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.19.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.6.1)\nRequirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (80.10.1)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.19.0)\nRequirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.8.0+cu126)\nRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (4.67.1)\nRequirement already satisfied: wget in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (3.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.0.0)\nRequirement already satisfied: braceexpand in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.1.7)\nRequirement already satisfied: editdistance in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.8.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.8.1)\nRequirement already satisfied: kaldi-python-io in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.2.2)\nRequirement already satisfied: lhotse==1.31.1 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.31.1)\nRequirement already satisfied: marshmallow in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (3.26.2)\nRequirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (4.6.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (24.2)\nRequirement already satisfied: pyannote.core in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (6.0.1)\nRequirement already satisfied: pyannote.metrics in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (3.2.1)\nRequirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.25.1)\nRequirement already satisfied: pyloudnorm in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.2.0)\nRequirement already satisfied: resampy in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.4.3)\nRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.15.3)\nRequirement already satisfied: sox<=1.5.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.5.0)\nRequirement already satisfied: kaldialign<=0.9.1 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.9.1)\nRequirement already satisfied: whisper_normalizer in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.1.12)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (4.4.2)\nRequirement already satisfied: inflect in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (7.5.0)\nRequirement already satisfied: mediapy==1.1.6 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.1.6)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (3.0.0)\nRequirement already satisfied: sacremoses>=0.0.43 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.1.1)\nRequirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.2.1)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (3.1.1)\nRequirement already satisfied: fiddle in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.3.0)\nRequirement already satisfied: hydra-core<=1.3.2,>1.3 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.3.2)\nRequirement already satisfied: lightning<=2.4.0,>2.2.1 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.4.0)\nRequirement already satisfied: omegaconf<=2.3 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.3.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.17.1)\nRequirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.8.2)\nRequirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.22.2)\nRequirement already satisfied: webdataset>=0.2.86 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.0.2)\nRequirement already satisfied: nv_one_logger_core>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.3.1)\nRequirement already satisfied: nv_one_logger_training_telemetry>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.3.1)\nRequirement already satisfied: nv_one_logger_pytorch_lightning_integration>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.3.1)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from lhotse==1.31.1->nemo_toolkit[asr]) (3.0.1)\nRequirement already satisfied: click>=7.1.1 in /usr/local/lib/python3.12/dist-packages (from lhotse==1.31.1->nemo_toolkit[asr]) (8.3.1)\nRequirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from lhotse==1.31.1->nemo_toolkit[asr]) (1.1.0)\nRequirement already satisfied: intervaltree>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from lhotse==1.31.1->nemo_toolkit[asr]) (3.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from lhotse==1.31.1->nemo_toolkit[asr]) (6.0.3)\nRequirement already satisfied: tabulate>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from lhotse==1.31.1->nemo_toolkit[asr]) (0.9.0)\nRequirement already satisfied: lilcom>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from lhotse==1.31.1->nemo_toolkit[asr]) (1.8.2)\nRequirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from mediapy==1.1.6->nemo_toolkit[asr]) (7.34.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapy==1.1.6->nemo_toolkit[asr]) (3.10.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from mediapy==1.1.6->nemo_toolkit[asr]) (11.3.0)\nRequirement already satisfied: numba>=0.59.1 in /usr/local/lib/python3.12/dist-packages (from numba-cuda==0.15.1->nemo_toolkit[asr]) (0.60.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.3)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\nRequirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.24->nemo_toolkit[asr]) (1.2.1rc0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core<=1.3.2,>1.3->nemo_toolkit[asr]) (4.9.3)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (0.15.2)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (from lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (2.6.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.59.1->numba-cuda==0.15.1->nemo_toolkit[asr]) (0.43.0)\nRequirement already satisfied: StrEnum<0.5.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (0.4.15)\nRequirement already satisfied: overrides<8.0.0,>=7.7.0 in /usr/local/lib/python3.12/dist-packages (from nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (7.7.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.10.6 in /usr/local/lib/python3.12/dist-packages (from nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (2.12.5)\nRequirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.12/dist-packages (from nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (0.10.2)\nRequirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.7.0->nemo_toolkit[asr]) (0.5.3)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->nemo_toolkit[asr]) (3.6.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (3.4.0)\nRequirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings->nemo_toolkit[asr]) (1.3.3)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (0.4.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (0.70.18)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from fiddle->nemo_toolkit[asr]) (1.4.0)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from fiddle->nemo_toolkit[asr]) (0.21)\nRequirement already satisfied: libcst in /usr/local/lib/python3.12/dist-packages (from fiddle->nemo_toolkit[asr]) (1.8.6)\nRequirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.12/dist-packages (from inflect->nemo_toolkit[asr]) (10.8.0)\nRequirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from inflect->nemo_toolkit[asr]) (4.4.4)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->nemo_toolkit[asr]) (1.17.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna->nemo_toolkit[asr]) (6.10.1)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->nemo_toolkit[asr]) (2.0.44)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->nemo_toolkit[asr]) (1.17.0)\nRequirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.core->nemo_toolkit[asr]) (2.4.0)\nRequirement already satisfied: pyannote.database>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics->nemo_toolkit[asr]) (6.1.1)\nRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics->nemo_toolkit[asr]) (0.6.2)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->nemo_toolkit[asr]) (1.75.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->nemo_toolkit[asr]) (3.9)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->nemo_toolkit[asr]) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->nemo_toolkit[asr]) (3.1.3)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->nemo_toolkit[asr]) (3.1.45)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->nemo_toolkit[asr]) (2.42.1)\nRequirement already satisfied: indic-numtowords in /usr/local/lib/python3.12/dist-packages (from whisper_normalizer->nemo_toolkit[asr]) (1.1.0)\nRequirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->nemo_toolkit[asr]) (1.3.10)\nRequirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from cytoolz>=0.10.1->lhotse==1.31.1->nemo_toolkit[asr]) (0.12.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (3.13.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[asr]) (4.0.12)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->nemo_toolkit[asr]) (4.12.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->nemo_toolkit[asr]) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets->nemo_toolkit[asr]) (0.16.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (1.3.3)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (4.60.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (1.4.9)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (3.2.5)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.6->nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.6->nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.6->nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (0.4.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->nemo_toolkit[asr]) (3.2.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->nemo_toolkit[asr]) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->nemo_toolkit[asr]) (3.0.3)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.19.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (3.0.52)\nRequirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (2.19.2)\nRequirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (4.9.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (1.22.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[asr]) (5.0.2)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.8.5)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.2.14)\nHit:1 https://cli.github.com/packages stable InRelease\nHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease     \nHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\nHit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease               \nHit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                     \nHit:6 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nHit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \nHit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nHit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\nHit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nHit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlibsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\nffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 139 not upgraded.\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (80.10.1)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import time\nimport gc\nimport torch\nimport numpy as np\nimport pandas as pd\nimport jiwer\nimport librosa\nimport soundfile as sf\nimport tempfile\nimport random\nimport os\nimport contextlib\nimport warnings\n\n# ==========================\n# SILENCE ALL WARNINGS / LOGS\n# ==========================\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import pipeline\nfrom transformers.utils import logging as hf_logging\nhf_logging.set_verbosity_error()\nhf_logging.disable_progress_bar()\n\nimport nemo.collections.asr as nemo_asr\nfrom sentence_transformers import SentenceTransformer, util\n\n\n# ==========================\n# CONFIG\n# ==========================\nDEVICE = 0 if torch.cuda.is_available() else -1\nDTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\nSEM_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n\n# ==========================\n# UTILS\n# ==========================\ndef add_noise(audio, noise_level=0.005):\n    noise = np.random.randn(len(audio))\n    return audio + noise_level * noise\n\n\ndef semantic_similarity(refs, preds):\n    ref_emb = SEM_MODEL.encode(refs, convert_to_tensor=True)\n    pred_emb = SEM_MODEL.encode(preds, convert_to_tensor=True)\n    sims = util.cos_sim(ref_emb, pred_emb).diagonal()\n    return float(sims.mean())\n\n\n# ==========================\n# BENCHMARK CLASS\n# ==========================\nclass STTBenchmark:\n\n    def load_librispeech_dataset(self, base_path, max_samples=1000):\n        from pathlib import Path\n\n        base_path = Path(base_path)\n        transcripts = {}\n        audio_pairs = []\n\n        for trans_file in base_path.rglob(\"*.trans.txt\"):\n            with open(trans_file) as f:\n                for line in f:\n                    uid, text = line.strip().split(\" \", 1)\n                    transcripts[uid] = text.strip()\n\n        for audio_file in base_path.rglob(\"*.flac\"):\n            uid = audio_file.stem\n            if uid in transcripts:\n                audio_pairs.append((audio_file, transcripts[uid]))\n\n        random.shuffle(audio_pairs)\n        audio_pairs = audio_pairs[:max_samples]\n\n        samples = []\n        for audio_file, text in audio_pairs:\n            audio, sr = librosa.load(str(audio_file), sr=16000)\n            samples.append({\n                \"audio\": audio,\n                \"reference\": text.upper(),\n                \"duration\": len(audio) / sr\n            })\n\n        print(f\"Loaded {len(samples)} random samples\")\n        return samples\n\n\n    # --------------------------\n    # TRANSFORMERS MODELS\n    # --------------------------\n    def eval_transformers(self, model_id, name, data, size, notes):\n        print(f\"\\nEvaluating: {name}\")\n\n        start = time.time()\n        pipe = pipeline(\n            \"automatic-speech-recognition\",\n            model=model_id,\n            device=DEVICE,\n            torch_dtype=DTYPE\n        )\n        load_time = time.time() - start\n\n        preds, refs, times, rtfs = [], [], [], []\n\n        for s in data:\n            t0 = time.time()\n\n            # silence stderr completely during inference\n            with open(os.devnull, \"w\") as fnull:\n                with contextlib.redirect_stderr(fnull):\n                    out = pipe(\n                        s[\"audio\"],\n                        chunk_length_s=30,\n                        stride_length_s=5\n                    )\n\n            t = time.time() - t0\n\n            pred = out[\"text\"].strip().upper()\n            preds.append(pred)\n            refs.append(s[\"reference\"])\n            times.append(t)\n            rtfs.append(t / s[\"duration\"])\n\n        fname = f\"predictions_{name.lower().replace(' ', '_')}.csv\"\n        pd.DataFrame({\"reference\": refs, \"prediction\": preds}).to_csv(fname, index=False)\n        print(f\"Saved predictions â†’ {fname}\")\n\n        wer = jiwer.wer(refs, preds)\n        sem = semantic_similarity(refs, preds)\n\n        noisy_preds = []\n        for s in data:\n            with open(os.devnull, \"w\") as fnull:\n                with contextlib.redirect_stderr(fnull):\n                    out = pipe(add_noise(s[\"audio\"]), chunk_length_s=30, stride_length_s=5)\n            noisy_preds.append(out[\"text\"].strip().upper())\n\n        robust = jiwer.wer(refs, noisy_preds)\n\n        del pipe\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        return {\n            \"model\": name,\n            \"wer\": wer * 100,\n            \"semantic_similarity\": sem,\n            \"avg_rtf\": float(np.mean(rtfs)),\n            \"robust_wer\": robust * 100,\n            \"load_time\": load_time,\n            \"avg_inference_time\": float(np.mean(times)),\n            \"total_inference_time\": float(sum(times)),\n            \"model_size\": size,\n            \"notes\": notes\n        }\n\n\n    # --------------------------\n    # CANARY (NEMO)\n    # --------------------------\n    def eval_canary_nemo(self, data):\n        print(\"\\nEvaluating: Canary 1B Flash (NeMo)\")\n\n        start = time.time()\n        model = nemo_asr.models.ASRModel.from_pretrained(\"nvidia/canary-1b-flash\")\n        if torch.cuda.is_available():\n            model = model.to(\"cuda\")\n        load_time = time.time() - start\n\n        preds, refs, times, rtfs = [], [], [], []\n\n        for s in data:\n            with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n                sf.write(tmp.name, s[\"audio\"], 16000)\n\n                t0 = time.time()\n                out = model.transcribe([tmp.name], source_lang=\"en\", target_lang=\"en\")\n                t = time.time() - t0\n\n                text = out[0] if isinstance(out[0], str) else out[0].text\n                preds.append(text.strip().upper())\n                refs.append(s[\"reference\"])\n                times.append(t)\n                rtfs.append(t / s[\"duration\"])\n\n        pd.DataFrame({\"reference\": refs, \"prediction\": preds}).to_csv(\n            \"predictions_canary_1b_flash.csv\", index=False\n        )\n        print(\"Saved predictions â†’ predictions_canary_1b_flash.csv\")\n\n        wer = jiwer.wer(refs, preds)\n        sem = semantic_similarity(refs, preds)\n\n        noisy_preds = []\n        for s in data:\n            with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n                sf.write(tmp.name, add_noise(s[\"audio\"]), 16000)\n                out = model.transcribe([tmp.name], source_lang=\"en\", target_lang=\"en\")\n                text = out[0] if isinstance(out[0], str) else out[0].text\n                noisy_preds.append(text.strip().upper())\n\n        robust = jiwer.wer(refs, noisy_preds)\n\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        return {\n            \"model\": \"Canary 1B Flash (NeMo)\",\n            \"wer\": wer * 100,\n            \"semantic_similarity\": sem,\n            \"avg_rtf\": float(np.mean(rtfs)),\n            \"robust_wer\": robust * 100,\n            \"load_time\": load_time,\n            \"avg_inference_time\": float(np.mean(times)),\n            \"total_inference_time\": float(sum(times)),\n            \"model_size\": \"~1.2GB\",\n            \"notes\": \"Official NVIDIA NeMo loader\"\n        }\n\n\n    # --------------------------\n    # RUN ALL\n    # --------------------------\n    def run(self, data):\n        results = []\n\n        results.append(self.eval_transformers(\"openai/whisper-base\", \"Whisper Base\", data, \"~140MB\", \"Fastest baseline\"))\n        results.append(self.eval_transformers(\"distil-whisper/distil-large-v3\", \"Distil-Whisper Large v3\", data, \"~756MB\", \"Best tradeoff\"))\n        results.append(self.eval_transformers(\"openai/whisper-large-v3\", \"Whisper Large v3\", data, \"~1.5GB\", \"Highest accuracy\"))\n        results.append(self.eval_transformers(\"facebook/wav2vec2-large-960h-lv60-self\", \"wav2vec2 Large\", data, \"~1.2GB\", \"SSL model\"))\n        results.append(self.eval_canary_nemo(data))\n\n        return pd.DataFrame(results)\n\n\n# ==========================\n# MAIN\n# ==========================\nif __name__ == \"__main__\":\n\n    benchmark = STTBenchmark()\n\n    test_data = benchmark.load_librispeech_dataset(\n        base_path=\"/kaggle/input/benchmark/LibriSpeech/test-clean\",\n        max_samples=1000\n    )\n\n    df = benchmark.run(test_data)\n\n    print(\"\\n================ RESULTS ================\")\n    print(df.to_string(index=False))\n\n    df.to_csv(\"stt_benchmark_results_extended.csv\", index=False)\n    print(\"\\nSaved â†’ stt_benchmark_results_extended.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T13:43:32.800236Z","iopub.execute_input":"2026-01-25T13:43:32.800762Z","iopub.status.idle":"2026-01-25T16:04:16.758208Z","shell.execute_reply.started":"2026-01-25T13:43:32.800736Z","shell.execute_reply":"2026-01-25T16:04:16.757438Z"}},"outputs":[{"name":"stdout","text":"Loaded 1000 random samples\n\nEvaluating: Whisper Base\nSaved predictions â†’ predictions_whisper_base.csv\n\nEvaluating: Distil-Whisper Large v3\nSaved predictions â†’ predictions_distil-whisper_large_v3.csv\n\nEvaluating: Whisper Large v3\nSaved predictions â†’ predictions_whisper_large_v3.csv\n\nEvaluating: wav2vec2 Large\nSaved predictions â†’ predictions_wav2vec2_large.csv\n\nEvaluating: Canary 1B Flash (NeMo)\n[NeMo I 2026-01-25 15:52:01 nemo_logging:393] _setup_tokenizer: detected an aggregate tokenizer\n[NeMo I 2026-01-25 15:52:01 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1152 tokens\n[NeMo I 2026-01-25 15:52:01 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n[NeMo I 2026-01-25 15:52:01 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n[NeMo I 2026-01-25 15:52:01 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n[NeMo I 2026-01-25 15:52:01 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n[NeMo I 2026-01-25 15:52:01 nemo_logging:393] Aggregate vocab size: 5248\n","output_type":"stream"},{"name":"stderr","text":"[NeMo W 2026-01-25 15:52:02 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n    Train config : \n    use_lhotse: true\n    input_cfg: null\n    tarred_audio_filepaths: null\n    manifest_filepath: null\n    sample_rate: 16000\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    prompt_format: canary2\n    max_tps: 25\n    max_duration: 40.0\n    text_field: answer\n    lang_field: target_lang\n    use_bucketing: true\n    bucket_duration_bins:\n    - - 3.971\n      - 30\n    - - 3.971\n      - 48\n    - - 4.973\n      - 37\n    - - 4.973\n      - 60\n    - - 5.85\n      - 42\n    - - 5.85\n      - 71\n    - - 6.56\n      - 46\n    - - 6.56\n      - 79\n    - - 7.32\n      - 49\n    - - 7.32\n      - 88\n    - - 8.19\n      - 54\n    - - 8.19\n      - 99\n    - - 8.88\n      - 61\n    - - 8.88\n      - 107\n    - - 9.76\n      - 66\n    - - 9.76\n      - 118\n    - - 10.56\n      - 72\n    - - 10.56\n      - 127\n    - - 11.214\n      - 76\n    - - 11.214\n      - 135\n    - - 11.867\n      - 79\n    - - 11.867\n      - 143\n    - - 12.53\n      - 82\n    - - 12.53\n      - 151\n    - - 13.08\n      - 87\n    - - 13.08\n      - 157\n    - - 13.62\n      - 91\n    - - 13.62\n      - 164\n    - - 14.16\n      - 93\n    - - 14.16\n      - 170\n    - - 14.7\n      - 96\n    - - 14.7\n      - 177\n    - - 15.2\n      - 99\n    - - 15.2\n      - 183\n    - - 15.68\n      - 101\n    - - 15.68\n      - 189\n    - - 16.135\n      - 102\n    - - 16.135\n      - 194\n    - - 16.67\n      - 105\n    - - 16.67\n      - 201\n    - - 17.197\n      - 108\n    - - 17.197\n      - 207\n    - - 17.73\n      - 111\n    - - 17.73\n      - 213\n    - - 18.2\n      - 114\n    - - 18.2\n      - 219\n    - - 18.69\n      - 117\n    - - 18.69\n      - 225\n    - - 19.153\n      - 120\n    - - 19.153\n      - 230\n    - - 19.63\n      - 123\n    - - 19.63\n      - 236\n    - - 20.44\n      - 122\n    - - 20.44\n      - 246\n    - - 32.567\n      - 174\n    - - 32.567\n      - 391\n    - - 36.587\n      - 227\n    - - 36.587\n      - 440\n    - - 40.0\n      - 253\n    - - 40.0\n      - 480\n    bucket_batch_size:\n    - 273\n    - 248\n    - 217\n    - 190\n    - 190\n    - 161\n    - 165\n    - 140\n    - 149\n    - 126\n    - 134\n    - 113\n    - 120\n    - 100\n    - 109\n    - 93\n    - 99\n    - 86\n    - 94\n    - 79\n    - 86\n    - 72\n    - 82\n    - 69\n    - 79\n    - 66\n    - 74\n    - 63\n    - 71\n    - 61\n    - 68\n    - 57\n    - 66\n    - 55\n    - 66\n    - 54\n    - 63\n    - 52\n    - 61\n    - 50\n    - 59\n    - 48\n    - 57\n    - 46\n    - 55\n    - 45\n    - 53\n    - 43\n    - 51\n    - 41\n    - 50\n    - 39\n    - 50\n    - 39\n    - 28\n    - 21\n    - 24\n    - 18\n    - 21\n    - 17\n    bucket_buffer_size: 40000\n    shuffle_buffer_size: 10000\n    concurrent_bucketing: false\n    augmentor: null\n    \n[NeMo W 2026-01-25 15:52:02 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n    Validation config : \n    use_lhotse: true\n    prompt_format: canary2\n    manifest_filepath: /data/ASR/en/librispeech/test-other.json\n    sample_rate: 16000\n    batch_size: 32\n    shuffle: false\n    num_workers: 2\n    pin_memory: true\n    text_field: answer\n    lang_field: target_lang\n    tarred_audio_filepaths: null\n    \n","output_type":"stream"},{"name":"stdout","text":"[NeMo I 2026-01-25 15:52:02 nemo_logging:393] PADDING: 0\n","output_type":"stream"},{"name":"stderr","text":"ERROR:hydra.utils:Error getting class at nemo.collections.asr.modules.transformer.get_nemo_transformer: Located non-class of type 'function' while loading 'nemo.collections.asr.modules.transformer.get_nemo_transformer'\n","output_type":"stream"},{"name":"stdout","text":"[NeMo I 2026-01-25 15:52:15 nemo_logging:393] Model EncDecMultiTaskModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--canary-1b-flash/snapshots/a9a55e0295e7dd50d0c8c2a19491900a0daf24f3/canary-1b-flash.nemo.\n","output_type":"stream"},{"name":"stderr","text":"[NeMo W 2026-01-25 15:52:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.39it/s]\n[NeMo W 2026-01-25 15:52:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.29it/s]\n[NeMo W 2026-01-25 15:52:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.09it/s]\n[NeMo W 2026-01-25 15:52:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.46it/s]\n[NeMo W 2026-01-25 15:52:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.54it/s]\n[NeMo W 2026-01-25 15:52:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.06it/s]\n[NeMo W 2026-01-25 15:52:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.30it/s]\n[NeMo W 2026-01-25 15:52:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.56it/s]\n[NeMo W 2026-01-25 15:52:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.37it/s]\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.03it/s]\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.10it/s]\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.34it/s]\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.45it/s]\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.67it/s]\n[NeMo W 2026-01-25 15:52:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.15it/s]\n[NeMo W 2026-01-25 15:52:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.73it/s]\n[NeMo W 2026-01-25 15:52:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.09it/s]\n[NeMo W 2026-01-25 15:52:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.82it/s]\n[NeMo W 2026-01-25 15:52:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.88it/s]\n[NeMo W 2026-01-25 15:52:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.23it/s]\n[NeMo W 2026-01-25 15:52:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.28it/s]\n[NeMo W 2026-01-25 15:52:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.87it/s]\n[NeMo W 2026-01-25 15:52:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.11it/s]\n[NeMo W 2026-01-25 15:52:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.24it/s]\n[NeMo W 2026-01-25 15:52:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.18it/s]\n[NeMo W 2026-01-25 15:52:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.23it/s]\n[NeMo W 2026-01-25 15:52:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.46it/s]\n[NeMo W 2026-01-25 15:52:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.10it/s]\n[NeMo W 2026-01-25 15:52:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.37it/s]\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.95it/s]\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.48it/s]\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.34it/s]\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.02it/s]\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.06it/s]\n[NeMo W 2026-01-25 15:52:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.07it/s]\n[NeMo W 2026-01-25 15:52:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.96it/s]\n[NeMo W 2026-01-25 15:52:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.34it/s]\n[NeMo W 2026-01-25 15:52:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.53it/s]\n[NeMo W 2026-01-25 15:52:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.76it/s]\n[NeMo W 2026-01-25 15:52:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.31it/s]\n[NeMo W 2026-01-25 15:52:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.34it/s]\n[NeMo W 2026-01-25 15:52:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.55it/s]\n[NeMo W 2026-01-25 15:52:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.56it/s]\n[NeMo W 2026-01-25 15:52:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.50it/s]\n[NeMo W 2026-01-25 15:52:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.68it/s]\n[NeMo W 2026-01-25 15:52:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.27it/s]\n[NeMo W 2026-01-25 15:52:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.39it/s]\n[NeMo W 2026-01-25 15:52:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.32it/s]\n[NeMo W 2026-01-25 15:52:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.87it/s]\n[NeMo W 2026-01-25 15:52:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.61it/s]\n[NeMo W 2026-01-25 15:52:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.57it/s]\n[NeMo W 2026-01-25 15:52:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.51it/s]\n[NeMo W 2026-01-25 15:52:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.73it/s]\n[NeMo W 2026-01-25 15:52:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.88it/s]\n[NeMo W 2026-01-25 15:52:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.05it/s]\n[NeMo W 2026-01-25 15:52:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.04it/s]\n[NeMo W 2026-01-25 15:52:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.26it/s]\n[NeMo W 2026-01-25 15:52:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.68it/s]\n[NeMo W 2026-01-25 15:52:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.86it/s]\n[NeMo W 2026-01-25 15:52:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.03it/s]\n[NeMo W 2026-01-25 15:52:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.47it/s]\n[NeMo W 2026-01-25 15:52:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.49it/s]\n[NeMo W 2026-01-25 15:52:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.66it/s]\n[NeMo W 2026-01-25 15:52:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.95it/s]\n[NeMo W 2026-01-25 15:52:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.44it/s]\n[NeMo W 2026-01-25 15:52:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.92it/s]\n[NeMo W 2026-01-25 15:52:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.58it/s]\n[NeMo W 2026-01-25 15:52:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.95it/s]\n[NeMo W 2026-01-25 15:52:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.23it/s]\n[NeMo W 2026-01-25 15:52:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.94it/s]\n[NeMo W 2026-01-25 15:52:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.74it/s]\n[NeMo W 2026-01-25 15:52:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.72it/s]\n[NeMo W 2026-01-25 15:52:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.24it/s]\n[NeMo W 2026-01-25 15:52:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.57it/s]\n[NeMo W 2026-01-25 15:52:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.90it/s]\n[NeMo W 2026-01-25 15:52:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.80it/s]\n[NeMo W 2026-01-25 15:52:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.96it/s]\n[NeMo W 2026-01-25 15:52:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.46it/s]\n[NeMo W 2026-01-25 15:52:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.98it/s]\n[NeMo W 2026-01-25 15:52:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.97it/s]\n[NeMo W 2026-01-25 15:52:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.34it/s]\n[NeMo W 2026-01-25 15:52:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.80it/s]\n[NeMo W 2026-01-25 15:52:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.76it/s]\n[NeMo W 2026-01-25 15:52:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.86it/s]\n[NeMo W 2026-01-25 15:52:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.30it/s]\n[NeMo W 2026-01-25 15:52:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.73it/s]\n[NeMo W 2026-01-25 15:52:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.11it/s]\n[NeMo W 2026-01-25 15:52:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.62it/s]\n[NeMo W 2026-01-25 15:52:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.98it/s]\n[NeMo W 2026-01-25 15:52:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.10it/s]\n[NeMo W 2026-01-25 15:52:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.18it/s]\n[NeMo W 2026-01-25 15:52:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.54it/s]\n[NeMo W 2026-01-25 15:52:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 15:52:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 15:52:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.25it/s]\n[NeMo W 2026-01-25 15:52:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.41it/s]\n[NeMo W 2026-01-25 15:52:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.73it/s]\n[NeMo W 2026-01-25 15:52:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.15it/s]\n[NeMo W 2026-01-25 15:52:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.92it/s]\n[NeMo W 2026-01-25 15:52:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.90it/s]\n[NeMo W 2026-01-25 15:52:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.12it/s]\n[NeMo W 2026-01-25 15:52:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.73it/s]\n[NeMo W 2026-01-25 15:52:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.37it/s]\n[NeMo W 2026-01-25 15:52:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.44it/s]\n[NeMo W 2026-01-25 15:52:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.56it/s]\n[NeMo W 2026-01-25 15:52:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.20it/s]\n[NeMo W 2026-01-25 15:52:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.25it/s]\n[NeMo W 2026-01-25 15:52:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.48it/s]\n[NeMo W 2026-01-25 15:52:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.94it/s]\n[NeMo W 2026-01-25 15:52:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.97it/s]\n[NeMo W 2026-01-25 15:52:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.79it/s]\n[NeMo W 2026-01-25 15:52:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.68it/s]\n[NeMo W 2026-01-25 15:52:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.27it/s]\n[NeMo W 2026-01-25 15:52:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.84it/s]\n[NeMo W 2026-01-25 15:52:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.77it/s]\n[NeMo W 2026-01-25 15:52:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.36it/s]\n[NeMo W 2026-01-25 15:52:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.75it/s]\n[NeMo W 2026-01-25 15:52:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.65it/s]\n[NeMo W 2026-01-25 15:52:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.49it/s]\n[NeMo W 2026-01-25 15:52:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.83it/s]\n[NeMo W 2026-01-25 15:52:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:52:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.98it/s]\n[NeMo W 2026-01-25 15:53:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.79it/s]\n[NeMo W 2026-01-25 15:53:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 15:53:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.41it/s]\n[NeMo W 2026-01-25 15:53:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.41it/s]\n[NeMo W 2026-01-25 15:53:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.73it/s]\n[NeMo W 2026-01-25 15:53:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.58it/s]\n[NeMo W 2026-01-25 15:53:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.04it/s]\n[NeMo W 2026-01-25 15:53:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.91it/s]\n[NeMo W 2026-01-25 15:53:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.10it/s]\n[NeMo W 2026-01-25 15:53:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 15:53:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.79it/s]\n[NeMo W 2026-01-25 15:53:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.81it/s]\n[NeMo W 2026-01-25 15:53:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.08it/s]\n[NeMo W 2026-01-25 15:53:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.07it/s]\n[NeMo W 2026-01-25 15:53:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.71it/s]\n[NeMo W 2026-01-25 15:53:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.16it/s]\n[NeMo W 2026-01-25 15:53:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.93it/s]\n[NeMo W 2026-01-25 15:53:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.89it/s]\n[NeMo W 2026-01-25 15:53:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.91it/s]\n[NeMo W 2026-01-25 15:53:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.52it/s]\n[NeMo W 2026-01-25 15:53:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.94it/s]\n[NeMo W 2026-01-25 15:53:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.55it/s]\n[NeMo W 2026-01-25 15:53:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.34it/s]\n[NeMo W 2026-01-25 15:53:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.25it/s]\n[NeMo W 2026-01-25 15:53:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.38it/s]\n[NeMo W 2026-01-25 15:53:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.92it/s]\n[NeMo W 2026-01-25 15:53:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.93it/s]\n[NeMo W 2026-01-25 15:53:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.70it/s]\n[NeMo W 2026-01-25 15:53:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.96it/s]\n[NeMo W 2026-01-25 15:53:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.79it/s]\n[NeMo W 2026-01-25 15:53:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.95it/s]\n[NeMo W 2026-01-25 15:53:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.50it/s]\n[NeMo W 2026-01-25 15:53:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.63it/s]\n[NeMo W 2026-01-25 15:53:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.59it/s]\n[NeMo W 2026-01-25 15:53:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.64it/s]\n[NeMo W 2026-01-25 15:53:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.32it/s]\n[NeMo W 2026-01-25 15:53:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.97it/s]\n[NeMo W 2026-01-25 15:53:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 15:53:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.27it/s]\n[NeMo W 2026-01-25 15:53:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.87it/s]\n[NeMo W 2026-01-25 15:53:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.00it/s]\n[NeMo W 2026-01-25 15:53:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.81it/s]\n[NeMo W 2026-01-25 15:53:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.56it/s]\n[NeMo W 2026-01-25 15:53:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.70it/s]\n[NeMo W 2026-01-25 15:53:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.42it/s]\n[NeMo W 2026-01-25 15:53:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.89it/s]\n[NeMo W 2026-01-25 15:53:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.17it/s]\n[NeMo W 2026-01-25 15:53:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.86it/s]\n[NeMo W 2026-01-25 15:53:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.74it/s]\n[NeMo W 2026-01-25 15:53:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.78it/s]\n[NeMo W 2026-01-25 15:53:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.80it/s]\n[NeMo W 2026-01-25 15:53:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.07it/s]\n[NeMo W 2026-01-25 15:53:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.17it/s]\n[NeMo W 2026-01-25 15:53:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.66it/s]\n[NeMo W 2026-01-25 15:53:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.25it/s]\n[NeMo W 2026-01-25 15:53:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.70it/s]\n[NeMo W 2026-01-25 15:53:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.86it/s]\n[NeMo W 2026-01-25 15:53:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.80it/s]\n[NeMo W 2026-01-25 15:53:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:01,  1.04s/it]\n[NeMo W 2026-01-25 15:53:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.61it/s]\n[NeMo W 2026-01-25 15:53:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.29it/s]\n[NeMo W 2026-01-25 15:53:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.46it/s]\n[NeMo W 2026-01-25 15:53:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.60it/s]\n[NeMo W 2026-01-25 15:53:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.32it/s]\n[NeMo W 2026-01-25 15:53:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.89it/s]\n[NeMo W 2026-01-25 15:53:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.01it/s]\n[NeMo W 2026-01-25 15:53:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.08it/s]\n[NeMo W 2026-01-25 15:53:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.85it/s]\n[NeMo W 2026-01-25 15:53:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.70it/s]\n[NeMo W 2026-01-25 15:53:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.69it/s]\n[NeMo W 2026-01-25 15:53:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.58it/s]\n[NeMo W 2026-01-25 15:53:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.64it/s]\n[NeMo W 2026-01-25 15:53:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.37it/s]\n[NeMo W 2026-01-25 15:53:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.55it/s]\n[NeMo W 2026-01-25 15:53:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.83it/s]\n[NeMo W 2026-01-25 15:53:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.02it/s]\n[NeMo W 2026-01-25 15:53:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.80it/s]\n[NeMo W 2026-01-25 15:53:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.51it/s]\n[NeMo W 2026-01-25 15:53:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.86it/s]\n[NeMo W 2026-01-25 15:53:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.26it/s]\n[NeMo W 2026-01-25 15:53:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.63it/s]\n[NeMo W 2026-01-25 15:53:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.99it/s]\n[NeMo W 2026-01-25 15:53:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 15:53:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.41it/s]\n[NeMo W 2026-01-25 15:53:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.72it/s]\n[NeMo W 2026-01-25 15:53:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.58it/s]\n[NeMo W 2026-01-25 15:53:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.47it/s]\n[NeMo W 2026-01-25 15:53:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.96it/s]\n[NeMo W 2026-01-25 15:53:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.33it/s]\n[NeMo W 2026-01-25 15:53:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.70it/s]\n[NeMo W 2026-01-25 15:53:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.80it/s]\n[NeMo W 2026-01-25 15:53:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.32it/s]\n[NeMo W 2026-01-25 15:53:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.79it/s]\n[NeMo W 2026-01-25 15:53:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.10it/s]\n[NeMo W 2026-01-25 15:53:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.39it/s]\n[NeMo W 2026-01-25 15:53:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.77it/s]\n[NeMo W 2026-01-25 15:53:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.77it/s]\n[NeMo W 2026-01-25 15:53:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.37it/s]\n[NeMo W 2026-01-25 15:53:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.54it/s]\n[NeMo W 2026-01-25 15:53:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.19it/s]\n[NeMo W 2026-01-25 15:53:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.11it/s]\n[NeMo W 2026-01-25 15:53:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.21it/s]\n[NeMo W 2026-01-25 15:53:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.38it/s]\n[NeMo W 2026-01-25 15:53:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.74it/s]\n[NeMo W 2026-01-25 15:53:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.94it/s]\n[NeMo W 2026-01-25 15:53:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.38it/s]\n[NeMo W 2026-01-25 15:53:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.85it/s]\n[NeMo W 2026-01-25 15:53:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.62it/s]\n[NeMo W 2026-01-25 15:53:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.49it/s]\n[NeMo W 2026-01-25 15:53:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.06it/s]\n[NeMo W 2026-01-25 15:53:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.98it/s]\n[NeMo W 2026-01-25 15:53:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.72it/s]\n[NeMo W 2026-01-25 15:53:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.90it/s]\n[NeMo W 2026-01-25 15:53:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.75it/s]\n[NeMo W 2026-01-25 15:53:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.54it/s]\n[NeMo W 2026-01-25 15:53:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.82it/s]\n[NeMo W 2026-01-25 15:53:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.80it/s]\n[NeMo W 2026-01-25 15:53:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.55it/s]\n[NeMo W 2026-01-25 15:53:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.02it/s]\n[NeMo W 2026-01-25 15:53:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.61it/s]\n[NeMo W 2026-01-25 15:53:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.48it/s]\n[NeMo W 2026-01-25 15:53:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.66it/s]\n[NeMo W 2026-01-25 15:53:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.80it/s]\n[NeMo W 2026-01-25 15:53:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.78it/s]\n[NeMo W 2026-01-25 15:53:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.38it/s]\n[NeMo W 2026-01-25 15:53:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.07it/s]\n[NeMo W 2026-01-25 15:53:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.78it/s]\n[NeMo W 2026-01-25 15:53:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.06it/s]\n[NeMo W 2026-01-25 15:53:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.41it/s]\n[NeMo W 2026-01-25 15:53:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.14it/s]\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.18it/s]\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.96it/s]\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.93it/s]\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.26it/s]\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.75it/s]\n[NeMo W 2026-01-25 15:53:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.40it/s]\n[NeMo W 2026-01-25 15:53:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.09it/s]\n[NeMo W 2026-01-25 15:53:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.55it/s]\n[NeMo W 2026-01-25 15:53:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.82it/s]\n[NeMo W 2026-01-25 15:53:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.51it/s]\n[NeMo W 2026-01-25 15:53:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.72it/s]\n[NeMo W 2026-01-25 15:53:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.45it/s]\n[NeMo W 2026-01-25 15:53:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.79it/s]\n[NeMo W 2026-01-25 15:53:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.52it/s]\n[NeMo W 2026-01-25 15:53:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.45it/s]\n[NeMo W 2026-01-25 15:53:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.88it/s]\n[NeMo W 2026-01-25 15:53:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.08it/s]\n[NeMo W 2026-01-25 15:53:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.53it/s]\n[NeMo W 2026-01-25 15:53:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.01it/s]\n[NeMo W 2026-01-25 15:53:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.75it/s]\n[NeMo W 2026-01-25 15:53:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.84it/s]\n[NeMo W 2026-01-25 15:53:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.72it/s]\n[NeMo W 2026-01-25 15:53:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 15:53:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.63it/s]\n[NeMo W 2026-01-25 15:53:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.60it/s]\n[NeMo W 2026-01-25 15:53:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.37it/s]\n[NeMo W 2026-01-25 15:53:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.37it/s]\n[NeMo W 2026-01-25 15:53:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.07it/s]\n[NeMo W 2026-01-25 15:53:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.86it/s]\n[NeMo W 2026-01-25 15:53:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.01it/s]\n[NeMo W 2026-01-25 15:53:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.61it/s]\n[NeMo W 2026-01-25 15:53:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:53:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.05it/s]\n[NeMo W 2026-01-25 15:54:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.24it/s]\n[NeMo W 2026-01-25 15:54:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.33it/s]\n[NeMo W 2026-01-25 15:54:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.57it/s]\n[NeMo W 2026-01-25 15:54:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.15it/s]\n[NeMo W 2026-01-25 15:54:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.12it/s]\n[NeMo W 2026-01-25 15:54:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.86it/s]\n[NeMo W 2026-01-25 15:54:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.23it/s]\n[NeMo W 2026-01-25 15:54:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.91it/s]\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.01it/s]\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.80it/s]\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.29it/s]\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.92it/s]\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.43it/s]\n[NeMo W 2026-01-25 15:54:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.63it/s]\n[NeMo W 2026-01-25 15:54:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.89it/s]\n[NeMo W 2026-01-25 15:54:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.58it/s]\n[NeMo W 2026-01-25 15:54:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.46it/s]\n[NeMo W 2026-01-25 15:54:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.54it/s]\n[NeMo W 2026-01-25 15:54:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.47it/s]\n[NeMo W 2026-01-25 15:54:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.03it/s]\n[NeMo W 2026-01-25 15:54:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.03it/s]\n[NeMo W 2026-01-25 15:54:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.05it/s]\n[NeMo W 2026-01-25 15:54:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.93it/s]\n[NeMo W 2026-01-25 15:54:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.86it/s]\n[NeMo W 2026-01-25 15:54:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.71it/s]\n[NeMo W 2026-01-25 15:54:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.07it/s]\n[NeMo W 2026-01-25 15:54:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.33it/s]\n[NeMo W 2026-01-25 15:54:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.31it/s]\n[NeMo W 2026-01-25 15:54:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.39it/s]\n[NeMo W 2026-01-25 15:54:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.79it/s]\n[NeMo W 2026-01-25 15:54:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.70it/s]\n[NeMo W 2026-01-25 15:54:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.07it/s]\n[NeMo W 2026-01-25 15:54:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.43it/s]\n[NeMo W 2026-01-25 15:54:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.86it/s]\n[NeMo W 2026-01-25 15:54:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.48it/s]\n[NeMo W 2026-01-25 15:54:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.77it/s]\n[NeMo W 2026-01-25 15:54:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.77it/s]\n[NeMo W 2026-01-25 15:54:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.24it/s]\n[NeMo W 2026-01-25 15:54:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.70it/s]\n[NeMo W 2026-01-25 15:54:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.14it/s]\n[NeMo W 2026-01-25 15:54:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.64it/s]\n[NeMo W 2026-01-25 15:54:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.49it/s]\n[NeMo W 2026-01-25 15:54:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.18it/s]\n[NeMo W 2026-01-25 15:54:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.14it/s]\n[NeMo W 2026-01-25 15:54:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.80it/s]\n[NeMo W 2026-01-25 15:54:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.79it/s]\n[NeMo W 2026-01-25 15:54:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.68it/s]\n[NeMo W 2026-01-25 15:54:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.38it/s]\n[NeMo W 2026-01-25 15:54:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.51it/s]\n[NeMo W 2026-01-25 15:54:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.40it/s]\n[NeMo W 2026-01-25 15:54:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.54it/s]\n[NeMo W 2026-01-25 15:54:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.29it/s]\n[NeMo W 2026-01-25 15:54:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.77it/s]\n[NeMo W 2026-01-25 15:54:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.56it/s]\n[NeMo W 2026-01-25 15:54:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.35it/s]\n[NeMo W 2026-01-25 15:54:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.88it/s]\n[NeMo W 2026-01-25 15:54:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.16it/s]\n[NeMo W 2026-01-25 15:54:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.25it/s]\n[NeMo W 2026-01-25 15:54:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.26it/s]\n[NeMo W 2026-01-25 15:54:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.24it/s]\n[NeMo W 2026-01-25 15:54:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.64it/s]\n[NeMo W 2026-01-25 15:54:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.54it/s]\n[NeMo W 2026-01-25 15:54:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.65it/s]\n[NeMo W 2026-01-25 15:54:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.51it/s]\n[NeMo W 2026-01-25 15:54:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.64it/s]\n[NeMo W 2026-01-25 15:54:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.75it/s]\n[NeMo W 2026-01-25 15:54:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.12it/s]\n[NeMo W 2026-01-25 15:54:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.66it/s]\n[NeMo W 2026-01-25 15:54:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.51it/s]\n[NeMo W 2026-01-25 15:54:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.59it/s]\n[NeMo W 2026-01-25 15:54:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.10it/s]\n[NeMo W 2026-01-25 15:54:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.96it/s]\n[NeMo W 2026-01-25 15:54:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.19it/s]\n[NeMo W 2026-01-25 15:54:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.39it/s]\n[NeMo W 2026-01-25 15:54:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.63it/s]\n[NeMo W 2026-01-25 15:54:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.32it/s]\n[NeMo W 2026-01-25 15:54:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.36it/s]\n[NeMo W 2026-01-25 15:54:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.70it/s]\n[NeMo W 2026-01-25 15:54:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.88it/s]\n[NeMo W 2026-01-25 15:54:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.71it/s]\n[NeMo W 2026-01-25 15:54:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.91it/s]\n[NeMo W 2026-01-25 15:54:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.30it/s]\n[NeMo W 2026-01-25 15:54:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.01it/s]\n[NeMo W 2026-01-25 15:54:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.11it/s]\n[NeMo W 2026-01-25 15:54:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.35it/s]\n[NeMo W 2026-01-25 15:54:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.87it/s]\n[NeMo W 2026-01-25 15:54:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.25it/s]\n[NeMo W 2026-01-25 15:54:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.46it/s]\n[NeMo W 2026-01-25 15:54:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.14it/s]\n[NeMo W 2026-01-25 15:54:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.88it/s]\n[NeMo W 2026-01-25 15:54:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.57it/s]\n[NeMo W 2026-01-25 15:54:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.41it/s]\n[NeMo W 2026-01-25 15:54:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.54it/s]\n[NeMo W 2026-01-25 15:54:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.82it/s]\n[NeMo W 2026-01-25 15:54:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.46it/s]\n[NeMo W 2026-01-25 15:54:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.49it/s]\n[NeMo W 2026-01-25 15:54:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 15:54:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.27it/s]\n[NeMo W 2026-01-25 15:54:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.65it/s]\n[NeMo W 2026-01-25 15:54:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.55it/s]\n[NeMo W 2026-01-25 15:54:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.87it/s]\n[NeMo W 2026-01-25 15:54:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.80it/s]\n[NeMo W 2026-01-25 15:54:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.95it/s]\n[NeMo W 2026-01-25 15:54:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.62it/s]\n[NeMo W 2026-01-25 15:54:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.94it/s]\n[NeMo W 2026-01-25 15:54:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.97it/s]\n[NeMo W 2026-01-25 15:54:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.86it/s]\n[NeMo W 2026-01-25 15:54:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.02it/s]\n[NeMo W 2026-01-25 15:54:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.29it/s]\n[NeMo W 2026-01-25 15:54:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.22it/s]\n[NeMo W 2026-01-25 15:54:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.92it/s]\n[NeMo W 2026-01-25 15:54:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.17it/s]\n[NeMo W 2026-01-25 15:54:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.68it/s]\n[NeMo W 2026-01-25 15:54:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.20it/s]\n[NeMo W 2026-01-25 15:54:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.23it/s]\n[NeMo W 2026-01-25 15:54:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.89it/s]\n[NeMo W 2026-01-25 15:54:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.57it/s]\n[NeMo W 2026-01-25 15:54:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.47it/s]\n[NeMo W 2026-01-25 15:54:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.76it/s]\n[NeMo W 2026-01-25 15:54:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.45it/s]\n[NeMo W 2026-01-25 15:54:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.43it/s]\n[NeMo W 2026-01-25 15:54:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.05it/s]\n[NeMo W 2026-01-25 15:54:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.30it/s]\n[NeMo W 2026-01-25 15:54:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.41it/s]\n[NeMo W 2026-01-25 15:54:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.33it/s]\n[NeMo W 2026-01-25 15:54:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.35it/s]\n[NeMo W 2026-01-25 15:54:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.70it/s]\n[NeMo W 2026-01-25 15:54:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.67it/s]\n[NeMo W 2026-01-25 15:54:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.99it/s]\n[NeMo W 2026-01-25 15:54:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.71it/s]\n[NeMo W 2026-01-25 15:54:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.04it/s]\n[NeMo W 2026-01-25 15:54:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.43it/s]\n[NeMo W 2026-01-25 15:54:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.84it/s]\n[NeMo W 2026-01-25 15:54:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.17it/s]\n[NeMo W 2026-01-25 15:54:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.25it/s]\n[NeMo W 2026-01-25 15:54:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.89it/s]\n[NeMo W 2026-01-25 15:54:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.70it/s]\n[NeMo W 2026-01-25 15:54:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.79it/s]\n[NeMo W 2026-01-25 15:54:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.10it/s]\n[NeMo W 2026-01-25 15:54:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.25it/s]\n[NeMo W 2026-01-25 15:54:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.49it/s]\n[NeMo W 2026-01-25 15:54:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.59it/s]\n[NeMo W 2026-01-25 15:54:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.70it/s]\n[NeMo W 2026-01-25 15:54:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.01it/s]\n[NeMo W 2026-01-25 15:54:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.95it/s]\n[NeMo W 2026-01-25 15:54:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.51it/s]\n[NeMo W 2026-01-25 15:54:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.36it/s]\n[NeMo W 2026-01-25 15:54:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.35it/s]\n[NeMo W 2026-01-25 15:54:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.75it/s]\n[NeMo W 2026-01-25 15:54:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.23it/s]\n[NeMo W 2026-01-25 15:54:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.45it/s]\n[NeMo W 2026-01-25 15:54:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.27it/s]\n[NeMo W 2026-01-25 15:54:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.98it/s]\n[NeMo W 2026-01-25 15:54:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.08it/s]\n[NeMo W 2026-01-25 15:54:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.61it/s]\n[NeMo W 2026-01-25 15:54:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.62it/s]\n[NeMo W 2026-01-25 15:54:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.92it/s]\n[NeMo W 2026-01-25 15:54:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.15it/s]\n[NeMo W 2026-01-25 15:54:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.38it/s]\n[NeMo W 2026-01-25 15:54:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.29it/s]\n[NeMo W 2026-01-25 15:54:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.38it/s]\n[NeMo W 2026-01-25 15:54:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.33it/s]\n[NeMo W 2026-01-25 15:54:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.97it/s]\n[NeMo W 2026-01-25 15:54:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.62it/s]\n[NeMo W 2026-01-25 15:54:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.97it/s]\n[NeMo W 2026-01-25 15:54:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.58it/s]\n[NeMo W 2026-01-25 15:54:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.13it/s]\n[NeMo W 2026-01-25 15:54:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.11it/s]\n[NeMo W 2026-01-25 15:54:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.23it/s]\n[NeMo W 2026-01-25 15:54:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:54:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.18it/s]\n[NeMo W 2026-01-25 15:55:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.31it/s]\n[NeMo W 2026-01-25 15:55:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.17it/s]\n[NeMo W 2026-01-25 15:55:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.76it/s]\n[NeMo W 2026-01-25 15:55:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 15:55:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.53it/s]\n[NeMo W 2026-01-25 15:55:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.45it/s]\n[NeMo W 2026-01-25 15:55:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.84it/s]\n[NeMo W 2026-01-25 15:55:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.13it/s]\n[NeMo W 2026-01-25 15:55:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.62it/s]\n[NeMo W 2026-01-25 15:55:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.99it/s]\n[NeMo W 2026-01-25 15:55:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.15it/s]\n[NeMo W 2026-01-25 15:55:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.08it/s]\n[NeMo W 2026-01-25 15:55:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.86it/s]\n[NeMo W 2026-01-25 15:55:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.74it/s]\n[NeMo W 2026-01-25 15:55:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.80it/s]\n[NeMo W 2026-01-25 15:55:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.83it/s]\n[NeMo W 2026-01-25 15:55:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.76it/s]\n[NeMo W 2026-01-25 15:55:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.77it/s]\n[NeMo W 2026-01-25 15:55:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.02it/s]\n[NeMo W 2026-01-25 15:55:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.47it/s]\n[NeMo W 2026-01-25 15:55:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.54it/s]\n[NeMo W 2026-01-25 15:55:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.61it/s]\n[NeMo W 2026-01-25 15:55:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.41it/s]\n[NeMo W 2026-01-25 15:55:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.09it/s]\n[NeMo W 2026-01-25 15:55:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.75it/s]\n[NeMo W 2026-01-25 15:55:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.89it/s]\n[NeMo W 2026-01-25 15:55:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.44it/s]\n[NeMo W 2026-01-25 15:55:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.63it/s]\n[NeMo W 2026-01-25 15:55:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.48it/s]\n[NeMo W 2026-01-25 15:55:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.90it/s]\n[NeMo W 2026-01-25 15:55:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.78it/s]\n[NeMo W 2026-01-25 15:55:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.36it/s]\n[NeMo W 2026-01-25 15:55:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.43it/s]\n[NeMo W 2026-01-25 15:55:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.04it/s]\n[NeMo W 2026-01-25 15:55:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.40it/s]\n[NeMo W 2026-01-25 15:55:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.08it/s]\n[NeMo W 2026-01-25 15:55:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.80it/s]\n[NeMo W 2026-01-25 15:55:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.01it/s]\n[NeMo W 2026-01-25 15:55:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.63it/s]\n[NeMo W 2026-01-25 15:55:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.36it/s]\n[NeMo W 2026-01-25 15:55:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.43it/s]\n[NeMo W 2026-01-25 15:55:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.22it/s]\n[NeMo W 2026-01-25 15:55:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.54it/s]\n[NeMo W 2026-01-25 15:55:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.53it/s]\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.94it/s]\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.63it/s]\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.74it/s]\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.93it/s]\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.47it/s]\n[NeMo W 2026-01-25 15:55:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.47it/s]\n[NeMo W 2026-01-25 15:55:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.28it/s]\n[NeMo W 2026-01-25 15:55:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.63it/s]\n[NeMo W 2026-01-25 15:55:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.39it/s]\n[NeMo W 2026-01-25 15:55:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.73it/s]\n[NeMo W 2026-01-25 15:55:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.12it/s]\n[NeMo W 2026-01-25 15:55:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.20it/s]\n[NeMo W 2026-01-25 15:55:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.93it/s]\n[NeMo W 2026-01-25 15:55:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.17it/s]\n[NeMo W 2026-01-25 15:55:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.53it/s]\n[NeMo W 2026-01-25 15:55:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.44it/s]\n[NeMo W 2026-01-25 15:55:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.84it/s]\n[NeMo W 2026-01-25 15:55:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.23it/s]\n[NeMo W 2026-01-25 15:55:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.26it/s]\n[NeMo W 2026-01-25 15:55:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.26it/s]\n[NeMo W 2026-01-25 15:55:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.49it/s]\n[NeMo W 2026-01-25 15:55:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.85it/s]\n[NeMo W 2026-01-25 15:55:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 15:55:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.66it/s]\n[NeMo W 2026-01-25 15:55:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.09it/s]\n[NeMo W 2026-01-25 15:55:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.77it/s]\n[NeMo W 2026-01-25 15:55:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.60it/s]\n[NeMo W 2026-01-25 15:55:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.93it/s]\n[NeMo W 2026-01-25 15:55:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.34it/s]\n[NeMo W 2026-01-25 15:55:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.33it/s]\n[NeMo W 2026-01-25 15:55:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.38it/s]\n[NeMo W 2026-01-25 15:55:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.33it/s]\n[NeMo W 2026-01-25 15:55:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.58it/s]\n[NeMo W 2026-01-25 15:55:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.59it/s]\n[NeMo W 2026-01-25 15:55:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.72it/s]\n[NeMo W 2026-01-25 15:55:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 15:55:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.65it/s]\n[NeMo W 2026-01-25 15:55:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.67it/s]\n[NeMo W 2026-01-25 15:55:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.45it/s]\n[NeMo W 2026-01-25 15:55:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.95it/s]\n[NeMo W 2026-01-25 15:55:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.32it/s]\n[NeMo W 2026-01-25 15:55:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.23it/s]\n[NeMo W 2026-01-25 15:55:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.57it/s]\n[NeMo W 2026-01-25 15:55:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.83it/s]\n[NeMo W 2026-01-25 15:55:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.10it/s]\n[NeMo W 2026-01-25 15:55:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.69it/s]\n[NeMo W 2026-01-25 15:55:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.15it/s]\n[NeMo W 2026-01-25 15:55:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.20it/s]\n[NeMo W 2026-01-25 15:55:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.81it/s]\n[NeMo W 2026-01-25 15:55:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.99it/s]\n[NeMo W 2026-01-25 15:55:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.40it/s]\n[NeMo W 2026-01-25 15:55:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.73it/s]\n[NeMo W 2026-01-25 15:55:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.06it/s]\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.61it/s]\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.26it/s]\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.90it/s]\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.83it/s]\n[NeMo W 2026-01-25 15:55:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.95it/s]\n[NeMo W 2026-01-25 15:55:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.11it/s]\n[NeMo W 2026-01-25 15:55:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.23it/s]\n[NeMo W 2026-01-25 15:55:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.05it/s]\n[NeMo W 2026-01-25 15:55:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.93it/s]\n[NeMo W 2026-01-25 15:55:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.10it/s]\n[NeMo W 2026-01-25 15:55:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.76it/s]\n[NeMo W 2026-01-25 15:55:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.65it/s]\n[NeMo W 2026-01-25 15:55:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.57it/s]\n[NeMo W 2026-01-25 15:55:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.39it/s]\n[NeMo W 2026-01-25 15:55:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.28it/s]\n[NeMo W 2026-01-25 15:55:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.23it/s]\n[NeMo W 2026-01-25 15:55:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.16it/s]\n[NeMo W 2026-01-25 15:55:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.26it/s]\n[NeMo W 2026-01-25 15:55:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.59it/s]\n[NeMo W 2026-01-25 15:55:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.57it/s]\n[NeMo W 2026-01-25 15:55:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.51it/s]\n[NeMo W 2026-01-25 15:55:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.18it/s]\n[NeMo W 2026-01-25 15:55:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.46it/s]\n[NeMo W 2026-01-25 15:55:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.90it/s]\n[NeMo W 2026-01-25 15:55:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.50it/s]\n[NeMo W 2026-01-25 15:55:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.00it/s]\n[NeMo W 2026-01-25 15:55:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.31it/s]\n[NeMo W 2026-01-25 15:55:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.62it/s]\n[NeMo W 2026-01-25 15:55:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.88it/s]\n[NeMo W 2026-01-25 15:55:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.09it/s]\n[NeMo W 2026-01-25 15:55:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.93it/s]\n[NeMo W 2026-01-25 15:55:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.38it/s]\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.17it/s]\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.48it/s]\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.43it/s]\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.38it/s]\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.04it/s]\n[NeMo W 2026-01-25 15:55:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.00it/s]\n[NeMo W 2026-01-25 15:55:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.68it/s]\n[NeMo W 2026-01-25 15:55:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.04it/s]\n[NeMo W 2026-01-25 15:55:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.47it/s]\n[NeMo W 2026-01-25 15:55:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.63it/s]\n[NeMo W 2026-01-25 15:55:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.82it/s]\n[NeMo W 2026-01-25 15:55:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.68it/s]\n[NeMo W 2026-01-25 15:55:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.19it/s]\n[NeMo W 2026-01-25 15:55:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.16it/s]\n[NeMo W 2026-01-25 15:55:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.44it/s]\n[NeMo W 2026-01-25 15:55:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.64it/s]\n[NeMo W 2026-01-25 15:55:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.95it/s]\n[NeMo W 2026-01-25 15:55:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.29it/s]\n[NeMo W 2026-01-25 15:55:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.37it/s]\n[NeMo W 2026-01-25 15:55:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.85it/s]\n[NeMo W 2026-01-25 15:55:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 15:55:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.52it/s]\n[NeMo W 2026-01-25 15:55:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.88it/s]\n[NeMo W 2026-01-25 15:55:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.37it/s]\n[NeMo W 2026-01-25 15:55:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.28it/s]\n[NeMo W 2026-01-25 15:55:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.43it/s]\n[NeMo W 2026-01-25 15:55:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00, 10.82it/s]\n[NeMo W 2026-01-25 15:55:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.85it/s]\n[NeMo W 2026-01-25 15:55:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.79it/s]\n[NeMo W 2026-01-25 15:55:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.89it/s]\n[NeMo W 2026-01-25 15:55:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.78it/s]\n[NeMo W 2026-01-25 15:55:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.23it/s]\n[NeMo W 2026-01-25 15:55:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.27it/s]\n[NeMo W 2026-01-25 15:55:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.85it/s]\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.09it/s]\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.28it/s]\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.00it/s]\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.55it/s]\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.42it/s]\n[NeMo W 2026-01-25 15:55:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.55it/s]\n[NeMo W 2026-01-25 15:55:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:55:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.01it/s]\n[NeMo W 2026-01-25 15:56:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.42it/s]\n[NeMo W 2026-01-25 15:56:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.24it/s]\n[NeMo W 2026-01-25 15:56:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.55it/s]\n[NeMo W 2026-01-25 15:56:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.39it/s]\n[NeMo W 2026-01-25 15:56:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.94it/s]\n[NeMo W 2026-01-25 15:56:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.84it/s]\n[NeMo W 2026-01-25 15:56:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.47it/s]\n[NeMo W 2026-01-25 15:56:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.98it/s]\n[NeMo W 2026-01-25 15:56:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.02it/s]\n[NeMo W 2026-01-25 15:56:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.45it/s]\n[NeMo W 2026-01-25 15:56:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.03it/s]\n[NeMo W 2026-01-25 15:56:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.29it/s]\n[NeMo W 2026-01-25 15:56:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.31it/s]\n[NeMo W 2026-01-25 15:56:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.09it/s]\n[NeMo W 2026-01-25 15:56:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.97it/s]\n[NeMo W 2026-01-25 15:56:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.86it/s]\n[NeMo W 2026-01-25 15:56:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.69it/s]\n[NeMo W 2026-01-25 15:56:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.77it/s]\n[NeMo W 2026-01-25 15:56:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.44it/s]\n[NeMo W 2026-01-25 15:56:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.07it/s]\n[NeMo W 2026-01-25 15:56:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.64it/s]\n[NeMo W 2026-01-25 15:56:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.22it/s]\n[NeMo W 2026-01-25 15:56:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.57it/s]\n[NeMo W 2026-01-25 15:56:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.79it/s]\n[NeMo W 2026-01-25 15:56:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:01,  1.00s/it]\n[NeMo W 2026-01-25 15:56:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.06it/s]\n[NeMo W 2026-01-25 15:56:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.58it/s]\n[NeMo W 2026-01-25 15:56:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.39it/s]\n[NeMo W 2026-01-25 15:56:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.50it/s]\n[NeMo W 2026-01-25 15:56:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.01it/s]\n[NeMo W 2026-01-25 15:56:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.61it/s]\n[NeMo W 2026-01-25 15:56:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.99it/s]\n[NeMo W 2026-01-25 15:56:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.15it/s]\n[NeMo W 2026-01-25 15:56:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.43it/s]\n[NeMo W 2026-01-25 15:56:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.43it/s]\n[NeMo W 2026-01-25 15:56:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.86it/s]\n[NeMo W 2026-01-25 15:56:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.81it/s]\n[NeMo W 2026-01-25 15:56:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.01it/s]\n[NeMo W 2026-01-25 15:56:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.49it/s]\n[NeMo W 2026-01-25 15:56:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 15:56:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.40it/s]\n[NeMo W 2026-01-25 15:56:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.40it/s]\n[NeMo W 2026-01-25 15:56:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.52it/s]\n[NeMo W 2026-01-25 15:56:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.77it/s]\n[NeMo W 2026-01-25 15:56:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.84it/s]\n[NeMo W 2026-01-25 15:56:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.51it/s]\n[NeMo W 2026-01-25 15:56:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.23it/s]\n[NeMo W 2026-01-25 15:56:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.94it/s]\n[NeMo W 2026-01-25 15:56:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.67it/s]\n[NeMo W 2026-01-25 15:56:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.99it/s]\n[NeMo W 2026-01-25 15:56:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.59it/s]\n[NeMo W 2026-01-25 15:56:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.29it/s]\n[NeMo W 2026-01-25 15:56:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.92it/s]\n[NeMo W 2026-01-25 15:56:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.76it/s]\n[NeMo W 2026-01-25 15:56:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.79it/s]\n[NeMo W 2026-01-25 15:56:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.92it/s]\n[NeMo W 2026-01-25 15:56:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.05it/s]\n[NeMo W 2026-01-25 15:56:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.22it/s]\n[NeMo W 2026-01-25 15:56:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.36it/s]\n[NeMo W 2026-01-25 15:56:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.32it/s]\n[NeMo W 2026-01-25 15:56:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.33it/s]\n[NeMo W 2026-01-25 15:56:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.14it/s]\n[NeMo W 2026-01-25 15:56:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.10it/s]\n[NeMo W 2026-01-25 15:56:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.75it/s]\n[NeMo W 2026-01-25 15:56:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.42it/s]\n[NeMo W 2026-01-25 15:56:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.30it/s]\n[NeMo W 2026-01-25 15:56:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.78it/s]\n[NeMo W 2026-01-25 15:56:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.54it/s]\n[NeMo W 2026-01-25 15:56:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.02it/s]\n[NeMo W 2026-01-25 15:56:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.08it/s]\n[NeMo W 2026-01-25 15:56:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.00it/s]\n[NeMo W 2026-01-25 15:56:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.04it/s]\n[NeMo W 2026-01-25 15:56:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.92it/s]\n[NeMo W 2026-01-25 15:56:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 15:56:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.03it/s]\n[NeMo W 2026-01-25 15:56:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.76it/s]\n[NeMo W 2026-01-25 15:56:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.17it/s]\n[NeMo W 2026-01-25 15:56:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.30it/s]\n[NeMo W 2026-01-25 15:56:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.66it/s]\n[NeMo W 2026-01-25 15:56:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.82it/s]\n[NeMo W 2026-01-25 15:56:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.74it/s]\n[NeMo W 2026-01-25 15:56:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.39it/s]\n[NeMo W 2026-01-25 15:56:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.29it/s]\n[NeMo W 2026-01-25 15:56:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.12it/s]\n[NeMo W 2026-01-25 15:56:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.22it/s]\n[NeMo W 2026-01-25 15:56:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.38it/s]\n[NeMo W 2026-01-25 15:56:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.52it/s]\n[NeMo W 2026-01-25 15:56:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.06it/s]\n[NeMo W 2026-01-25 15:56:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.29it/s]\n[NeMo W 2026-01-25 15:56:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.94it/s]\n[NeMo W 2026-01-25 15:56:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.70it/s]\n[NeMo W 2026-01-25 15:56:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.35it/s]\n[NeMo W 2026-01-25 15:56:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.23it/s]\n[NeMo W 2026-01-25 15:56:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.92it/s]\n[NeMo W 2026-01-25 15:56:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.10it/s]\n[NeMo W 2026-01-25 15:56:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.65it/s]\n[NeMo W 2026-01-25 15:56:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.67it/s]\n[NeMo W 2026-01-25 15:56:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.13it/s]\n[NeMo W 2026-01-25 15:56:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.71it/s]\n[NeMo W 2026-01-25 15:56:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.16it/s]\n[NeMo W 2026-01-25 15:56:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.72it/s]\n[NeMo W 2026-01-25 15:56:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.58it/s]\n[NeMo W 2026-01-25 15:56:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.35it/s]\n[NeMo W 2026-01-25 15:56:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.60it/s]\n[NeMo W 2026-01-25 15:56:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.77it/s]\n[NeMo W 2026-01-25 15:56:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.92it/s]\n[NeMo W 2026-01-25 15:56:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.47it/s]\n[NeMo W 2026-01-25 15:56:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.80it/s]\n[NeMo W 2026-01-25 15:56:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.64it/s]\n[NeMo W 2026-01-25 15:56:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.72it/s]\n[NeMo W 2026-01-25 15:56:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.97it/s]\n[NeMo W 2026-01-25 15:56:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.45it/s]\n[NeMo W 2026-01-25 15:56:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.10it/s]\n[NeMo W 2026-01-25 15:56:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.37it/s]\n[NeMo W 2026-01-25 15:56:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.46it/s]\n[NeMo W 2026-01-25 15:56:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.10it/s]\n[NeMo W 2026-01-25 15:56:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.31it/s]\n[NeMo W 2026-01-25 15:56:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.92it/s]\n[NeMo W 2026-01-25 15:56:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.72it/s]\n[NeMo W 2026-01-25 15:56:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.36it/s]\n[NeMo W 2026-01-25 15:56:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.33it/s]\n[NeMo W 2026-01-25 15:56:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.90it/s]\n[NeMo W 2026-01-25 15:56:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.67it/s]\n[NeMo W 2026-01-25 15:56:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.61it/s]\n[NeMo W 2026-01-25 15:56:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.39it/s]\n[NeMo W 2026-01-25 15:56:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.40it/s]\n[NeMo W 2026-01-25 15:56:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.69it/s]\n[NeMo W 2026-01-25 15:56:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.12it/s]\n[NeMo W 2026-01-25 15:56:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.32it/s]\n[NeMo W 2026-01-25 15:56:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.35it/s]\n[NeMo W 2026-01-25 15:56:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.69it/s]\n[NeMo W 2026-01-25 15:56:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.55it/s]\n[NeMo W 2026-01-25 15:56:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.01it/s]\n[NeMo W 2026-01-25 15:56:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.16it/s]\n[NeMo W 2026-01-25 15:56:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.23it/s]\n[NeMo W 2026-01-25 15:56:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.96it/s]\n[NeMo W 2026-01-25 15:56:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.78it/s]\n[NeMo W 2026-01-25 15:56:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.89it/s]\n[NeMo W 2026-01-25 15:56:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.22it/s]\n[NeMo W 2026-01-25 15:56:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.40it/s]\n[NeMo W 2026-01-25 15:56:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.51it/s]\n[NeMo W 2026-01-25 15:56:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.29it/s]\n[NeMo W 2026-01-25 15:56:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.25it/s]\n[NeMo W 2026-01-25 15:56:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.70it/s]\n[NeMo W 2026-01-25 15:56:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.49it/s]\n[NeMo W 2026-01-25 15:56:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.04it/s]\n[NeMo W 2026-01-25 15:56:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.69it/s]\n[NeMo W 2026-01-25 15:56:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.05it/s]\n[NeMo W 2026-01-25 15:56:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.54it/s]\n[NeMo W 2026-01-25 15:56:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.82it/s]\n[NeMo W 2026-01-25 15:56:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.32it/s]\n[NeMo W 2026-01-25 15:56:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.47it/s]\n[NeMo W 2026-01-25 15:56:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.75it/s]\n[NeMo W 2026-01-25 15:56:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.45it/s]\n[NeMo W 2026-01-25 15:56:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.66it/s]\n[NeMo W 2026-01-25 15:56:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.73it/s]\n[NeMo W 2026-01-25 15:56:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.73it/s]\n[NeMo W 2026-01-25 15:56:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.60it/s]\n[NeMo W 2026-01-25 15:56:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.60it/s]\n[NeMo W 2026-01-25 15:56:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.93it/s]\n[NeMo W 2026-01-25 15:56:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:56:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.13it/s]\n[NeMo W 2026-01-25 15:57:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.93it/s]\n[NeMo W 2026-01-25 15:57:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.84it/s]\n[NeMo W 2026-01-25 15:57:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.66it/s]\n[NeMo W 2026-01-25 15:57:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.98it/s]\n[NeMo W 2026-01-25 15:57:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.11it/s]\n[NeMo W 2026-01-25 15:57:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 15:57:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.99it/s]\n[NeMo W 2026-01-25 15:57:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.80it/s]\n[NeMo W 2026-01-25 15:57:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.56it/s]\n[NeMo W 2026-01-25 15:57:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.38it/s]\n[NeMo W 2026-01-25 15:57:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.48it/s]\n[NeMo W 2026-01-25 15:57:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.26it/s]\n[NeMo W 2026-01-25 15:57:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.73it/s]\n[NeMo W 2026-01-25 15:57:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.68it/s]\n[NeMo W 2026-01-25 15:57:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.86it/s]\n[NeMo W 2026-01-25 15:57:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.03it/s]\n[NeMo W 2026-01-25 15:57:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.79it/s]\n[NeMo W 2026-01-25 15:57:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.35it/s]\n[NeMo W 2026-01-25 15:57:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.73it/s]\n[NeMo W 2026-01-25 15:57:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.27it/s]\n[NeMo W 2026-01-25 15:57:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.63it/s]\n[NeMo W 2026-01-25 15:57:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.39it/s]\n[NeMo W 2026-01-25 15:57:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.49it/s]\n[NeMo W 2026-01-25 15:57:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.99it/s]\n[NeMo W 2026-01-25 15:57:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.94it/s]\n[NeMo W 2026-01-25 15:57:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.35it/s]\n[NeMo W 2026-01-25 15:57:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.22it/s]\n[NeMo W 2026-01-25 15:57:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.90it/s]\n[NeMo W 2026-01-25 15:57:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.05it/s]\n[NeMo W 2026-01-25 15:57:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.21it/s]\n[NeMo W 2026-01-25 15:57:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.93it/s]\n[NeMo W 2026-01-25 15:57:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.25it/s]\n[NeMo W 2026-01-25 15:57:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.82it/s]\n[NeMo W 2026-01-25 15:57:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.23it/s]\n[NeMo W 2026-01-25 15:57:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.14it/s]\n[NeMo W 2026-01-25 15:57:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.39it/s]\n[NeMo W 2026-01-25 15:57:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.60it/s]\n[NeMo W 2026-01-25 15:57:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.58it/s]\n[NeMo W 2026-01-25 15:57:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.36it/s]\n[NeMo W 2026-01-25 15:57:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.32it/s]\n[NeMo W 2026-01-25 15:57:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.36it/s]\n[NeMo W 2026-01-25 15:57:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.08it/s]\n[NeMo W 2026-01-25 15:57:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.80it/s]\n[NeMo W 2026-01-25 15:57:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.77it/s]\n[NeMo W 2026-01-25 15:57:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.98it/s]\n[NeMo W 2026-01-25 15:57:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.16it/s]\n[NeMo W 2026-01-25 15:57:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.81it/s]\n[NeMo W 2026-01-25 15:57:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.73it/s]\n[NeMo W 2026-01-25 15:57:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.04it/s]\n[NeMo W 2026-01-25 15:57:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.89it/s]\n[NeMo W 2026-01-25 15:57:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.29it/s]\n[NeMo W 2026-01-25 15:57:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.54it/s]\n[NeMo W 2026-01-25 15:57:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.04it/s]\n[NeMo W 2026-01-25 15:57:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.13it/s]\n[NeMo W 2026-01-25 15:57:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.13it/s]\n[NeMo W 2026-01-25 15:57:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.88it/s]\n[NeMo W 2026-01-25 15:57:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.97it/s]\n[NeMo W 2026-01-25 15:57:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.24it/s]\n[NeMo W 2026-01-25 15:57:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.74it/s]\n[NeMo W 2026-01-25 15:57:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.82it/s]\n[NeMo W 2026-01-25 15:57:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.86it/s]\n[NeMo W 2026-01-25 15:57:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.07it/s]\n[NeMo W 2026-01-25 15:57:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 15:57:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.36it/s]\n[NeMo W 2026-01-25 15:57:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.61it/s]\n[NeMo W 2026-01-25 15:57:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.77it/s]\n[NeMo W 2026-01-25 15:57:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.28it/s]\n[NeMo W 2026-01-25 15:57:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.03it/s]\n[NeMo W 2026-01-25 15:57:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.01it/s]\n[NeMo W 2026-01-25 15:57:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.98it/s]\n[NeMo W 2026-01-25 15:57:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.03it/s]\n[NeMo W 2026-01-25 15:57:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.33it/s]\n[NeMo W 2026-01-25 15:57:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.13it/s]\n[NeMo W 2026-01-25 15:57:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.19it/s]\n[NeMo W 2026-01-25 15:57:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.05it/s]\n[NeMo W 2026-01-25 15:57:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.94it/s]\n[NeMo W 2026-01-25 15:57:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.57it/s]\n[NeMo W 2026-01-25 15:57:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.06it/s]\n[NeMo W 2026-01-25 15:57:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.31it/s]\n[NeMo W 2026-01-25 15:57:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.29it/s]\n[NeMo W 2026-01-25 15:57:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.74it/s]\n[NeMo W 2026-01-25 15:57:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.84it/s]\n[NeMo W 2026-01-25 15:57:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.76it/s]\n[NeMo W 2026-01-25 15:57:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.74it/s]\n[NeMo W 2026-01-25 15:57:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.56it/s]\n[NeMo W 2026-01-25 15:57:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.24it/s]\n[NeMo W 2026-01-25 15:57:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.29it/s]\n[NeMo W 2026-01-25 15:57:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.86it/s]\n[NeMo W 2026-01-25 15:57:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.16it/s]\n[NeMo W 2026-01-25 15:57:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.23it/s]\n[NeMo W 2026-01-25 15:57:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.12it/s]\n[NeMo W 2026-01-25 15:57:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.16it/s]\n[NeMo W 2026-01-25 15:57:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.13it/s]\n[NeMo W 2026-01-25 15:57:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.32it/s]\n[NeMo W 2026-01-25 15:57:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.92it/s]\n[NeMo W 2026-01-25 15:57:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.61it/s]\n[NeMo W 2026-01-25 15:57:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.62it/s]\n[NeMo W 2026-01-25 15:57:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.18it/s]\n[NeMo W 2026-01-25 15:57:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.05it/s]\n[NeMo W 2026-01-25 15:57:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.21it/s]\n[NeMo W 2026-01-25 15:57:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.92it/s]\n[NeMo W 2026-01-25 15:57:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.93it/s]\n[NeMo W 2026-01-25 15:57:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.73it/s]\n[NeMo W 2026-01-25 15:57:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.59it/s]\n[NeMo W 2026-01-25 15:57:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.99it/s]\n[NeMo W 2026-01-25 15:57:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.94it/s]\n[NeMo W 2026-01-25 15:57:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.10it/s]\n[NeMo W 2026-01-25 15:57:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.94it/s]\n[NeMo W 2026-01-25 15:57:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.36it/s]\n[NeMo W 2026-01-25 15:57:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.30it/s]\n[NeMo W 2026-01-25 15:57:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.80it/s]\n[NeMo W 2026-01-25 15:57:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.94it/s]\n[NeMo W 2026-01-25 15:57:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.57it/s]\n[NeMo W 2026-01-25 15:57:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.06it/s]\n[NeMo W 2026-01-25 15:57:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.29it/s]\n[NeMo W 2026-01-25 15:57:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.19it/s]\n[NeMo W 2026-01-25 15:57:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.87it/s]\n[NeMo W 2026-01-25 15:57:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.66it/s]\n[NeMo W 2026-01-25 15:57:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.47it/s]\n[NeMo W 2026-01-25 15:57:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.34it/s]\n[NeMo W 2026-01-25 15:57:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.69it/s]\n[NeMo W 2026-01-25 15:57:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.47it/s]\n[NeMo W 2026-01-25 15:57:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.15it/s]\n[NeMo W 2026-01-25 15:57:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.80it/s]\n[NeMo W 2026-01-25 15:57:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.66it/s]\n[NeMo W 2026-01-25 15:57:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.74it/s]\n[NeMo W 2026-01-25 15:57:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.38it/s]\n[NeMo W 2026-01-25 15:57:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.07it/s]\n[NeMo W 2026-01-25 15:57:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.58it/s]\n[NeMo W 2026-01-25 15:57:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.74it/s]\n[NeMo W 2026-01-25 15:57:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.26it/s]\n[NeMo W 2026-01-25 15:57:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.92it/s]\n[NeMo W 2026-01-25 15:57:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.30it/s]\n[NeMo W 2026-01-25 15:57:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.94it/s]\n[NeMo W 2026-01-25 15:57:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.23it/s]\n[NeMo W 2026-01-25 15:57:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.68it/s]\n[NeMo W 2026-01-25 15:57:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.18it/s]\n[NeMo W 2026-01-25 15:57:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.07it/s]\n[NeMo W 2026-01-25 15:57:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.51it/s]\n[NeMo W 2026-01-25 15:57:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.98it/s]\n[NeMo W 2026-01-25 15:57:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.86it/s]\n[NeMo W 2026-01-25 15:57:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.63it/s]\n[NeMo W 2026-01-25 15:57:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.32it/s]\n[NeMo W 2026-01-25 15:57:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.77it/s]\n[NeMo W 2026-01-25 15:57:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.74it/s]\n[NeMo W 2026-01-25 15:57:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.58it/s]\n[NeMo W 2026-01-25 15:57:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.09it/s]\n[NeMo W 2026-01-25 15:57:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.28it/s]\n[NeMo W 2026-01-25 15:57:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.21it/s]\n[NeMo W 2026-01-25 15:57:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.28it/s]\n[NeMo W 2026-01-25 15:57:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.61it/s]\n[NeMo W 2026-01-25 15:57:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.85it/s]\n[NeMo W 2026-01-25 15:57:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.17it/s]\n[NeMo W 2026-01-25 15:57:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.93it/s]\n[NeMo W 2026-01-25 15:57:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.63it/s]\n[NeMo W 2026-01-25 15:57:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.12it/s]\n[NeMo W 2026-01-25 15:57:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.89it/s]\n[NeMo W 2026-01-25 15:57:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00, 11.15it/s]\n[NeMo W 2026-01-25 15:57:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.56it/s]\n[NeMo W 2026-01-25 15:57:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.34it/s]\n[NeMo W 2026-01-25 15:57:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.88it/s]\n[NeMo W 2026-01-25 15:57:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.99it/s]\n[NeMo W 2026-01-25 15:57:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.91it/s]\n[NeMo W 2026-01-25 15:57:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.06it/s]\n[NeMo W 2026-01-25 15:57:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.92it/s]\n[NeMo W 2026-01-25 15:57:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.48it/s]\n[NeMo W 2026-01-25 15:57:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.21it/s]\n[NeMo W 2026-01-25 15:57:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.85it/s]\n[NeMo W 2026-01-25 15:57:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.54it/s]\n[NeMo W 2026-01-25 15:57:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.54it/s]\n[NeMo W 2026-01-25 15:57:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.64it/s]\n[NeMo W 2026-01-25 15:57:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:57:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.93it/s]\n[NeMo W 2026-01-25 15:58:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.58it/s]\n[NeMo W 2026-01-25 15:58:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.29it/s]\n[NeMo W 2026-01-25 15:58:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.00it/s]\n[NeMo W 2026-01-25 15:58:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.75it/s]\n[NeMo W 2026-01-25 15:58:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.99it/s]\n[NeMo W 2026-01-25 15:58:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.11it/s]\n[NeMo W 2026-01-25 15:58:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.58it/s]\n[NeMo W 2026-01-25 15:58:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.69it/s]\n[NeMo W 2026-01-25 15:58:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.38it/s]\n[NeMo W 2026-01-25 15:58:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.44it/s]\n[NeMo W 2026-01-25 15:58:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.19it/s]\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.25it/s]\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.62it/s]\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.47it/s]\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.48it/s]\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.85it/s]\n[NeMo W 2026-01-25 15:58:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.87it/s]\n[NeMo W 2026-01-25 15:58:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.06it/s]\n[NeMo W 2026-01-25 15:58:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.46it/s]\n[NeMo W 2026-01-25 15:58:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.46it/s]\n[NeMo W 2026-01-25 15:58:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.51it/s]\n[NeMo W 2026-01-25 15:58:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.97it/s]\n[NeMo W 2026-01-25 15:58:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.61it/s]\n[NeMo W 2026-01-25 15:58:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.18it/s]\n[NeMo W 2026-01-25 15:58:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.63it/s]\n[NeMo W 2026-01-25 15:58:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.38it/s]\n[NeMo W 2026-01-25 15:58:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.01it/s]\n[NeMo W 2026-01-25 15:58:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.42it/s]\n[NeMo W 2026-01-25 15:58:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.77it/s]\n[NeMo W 2026-01-25 15:58:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.55it/s]\n[NeMo W 2026-01-25 15:58:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.57it/s]\n[NeMo W 2026-01-25 15:58:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.65it/s]\n[NeMo W 2026-01-25 15:58:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.52it/s]\n[NeMo W 2026-01-25 15:58:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.78it/s]\n[NeMo W 2026-01-25 15:58:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.60it/s]\n[NeMo W 2026-01-25 15:58:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.45it/s]\n[NeMo W 2026-01-25 15:58:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.53it/s]\n[NeMo W 2026-01-25 15:58:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.13it/s]\n[NeMo W 2026-01-25 15:58:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.71it/s]\n[NeMo W 2026-01-25 15:58:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.15it/s]\n[NeMo W 2026-01-25 15:58:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.57it/s]\n[NeMo W 2026-01-25 15:58:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.07it/s]\n[NeMo W 2026-01-25 15:58:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved predictions â†’ predictions_canary_1b_flash.csv\n","output_type":"stream"},{"name":"stderr","text":"[NeMo W 2026-01-25 15:58:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.10it/s]\n[NeMo W 2026-01-25 15:58:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.23it/s]\n[NeMo W 2026-01-25 15:58:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.10it/s]\n[NeMo W 2026-01-25 15:58:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.50it/s]\n[NeMo W 2026-01-25 15:58:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.96it/s]\n[NeMo W 2026-01-25 15:58:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.23it/s]\n[NeMo W 2026-01-25 15:58:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.12it/s]\n[NeMo W 2026-01-25 15:58:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.69it/s]\n[NeMo W 2026-01-25 15:58:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.30it/s]\n[NeMo W 2026-01-25 15:58:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.21it/s]\n[NeMo W 2026-01-25 15:58:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.35it/s]\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.64it/s]\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.60it/s]\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.72it/s]\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.03it/s]\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.03it/s]\n[NeMo W 2026-01-25 15:58:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.20it/s]\n[NeMo W 2026-01-25 15:58:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.87it/s]\n[NeMo W 2026-01-25 15:58:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.82it/s]\n[NeMo W 2026-01-25 15:58:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.95it/s]\n[NeMo W 2026-01-25 15:58:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.07it/s]\n[NeMo W 2026-01-25 15:58:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.16it/s]\n[NeMo W 2026-01-25 15:58:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.19it/s]\n[NeMo W 2026-01-25 15:58:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.54it/s]\n[NeMo W 2026-01-25 15:58:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.48it/s]\n[NeMo W 2026-01-25 15:58:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.38it/s]\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.67it/s]\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.97it/s]\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.21it/s]\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.17it/s]\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.80it/s]\n[NeMo W 2026-01-25 15:58:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.51it/s]\n[NeMo W 2026-01-25 15:58:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.26it/s]\n[NeMo W 2026-01-25 15:58:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.03it/s]\n[NeMo W 2026-01-25 15:58:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.23it/s]\n[NeMo W 2026-01-25 15:58:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.25it/s]\n[NeMo W 2026-01-25 15:58:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.20it/s]\n[NeMo W 2026-01-25 15:58:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.61it/s]\n[NeMo W 2026-01-25 15:58:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.76it/s]\n[NeMo W 2026-01-25 15:58:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.16it/s]\n[NeMo W 2026-01-25 15:58:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.09it/s]\n[NeMo W 2026-01-25 15:58:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.56it/s]\n[NeMo W 2026-01-25 15:58:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.54it/s]\n[NeMo W 2026-01-25 15:58:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.80it/s]\n[NeMo W 2026-01-25 15:58:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.63it/s]\n[NeMo W 2026-01-25 15:58:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.29it/s]\n[NeMo W 2026-01-25 15:58:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.32it/s]\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.39it/s]\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.90it/s]\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.58it/s]\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.46it/s]\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.30it/s]\n[NeMo W 2026-01-25 15:58:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.75it/s]\n[NeMo W 2026-01-25 15:58:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.06it/s]\n[NeMo W 2026-01-25 15:58:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.24it/s]\n[NeMo W 2026-01-25 15:58:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.05it/s]\n[NeMo W 2026-01-25 15:58:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.27it/s]\n[NeMo W 2026-01-25 15:58:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.60it/s]\n[NeMo W 2026-01-25 15:58:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.88it/s]\n[NeMo W 2026-01-25 15:58:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.99it/s]\n[NeMo W 2026-01-25 15:58:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.38it/s]\n[NeMo W 2026-01-25 15:58:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.50it/s]\n[NeMo W 2026-01-25 15:58:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.66it/s]\n[NeMo W 2026-01-25 15:58:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.01it/s]\n[NeMo W 2026-01-25 15:58:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.18it/s]\n[NeMo W 2026-01-25 15:58:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.62it/s]\n[NeMo W 2026-01-25 15:58:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.72it/s]\n[NeMo W 2026-01-25 15:58:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.17it/s]\n[NeMo W 2026-01-25 15:58:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.27it/s]\n[NeMo W 2026-01-25 15:58:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.78it/s]\n[NeMo W 2026-01-25 15:58:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.75it/s]\n[NeMo W 2026-01-25 15:58:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.74it/s]\n[NeMo W 2026-01-25 15:58:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.33it/s]\n[NeMo W 2026-01-25 15:58:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.84it/s]\n[NeMo W 2026-01-25 15:58:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.92it/s]\n[NeMo W 2026-01-25 15:58:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.88it/s]\n[NeMo W 2026-01-25 15:58:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.97it/s]\n[NeMo W 2026-01-25 15:58:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.50it/s]\n[NeMo W 2026-01-25 15:58:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.99it/s]\n[NeMo W 2026-01-25 15:58:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.00it/s]\n[NeMo W 2026-01-25 15:58:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.33it/s]\n[NeMo W 2026-01-25 15:58:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 15:58:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 15:58:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.90it/s]\n[NeMo W 2026-01-25 15:58:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.27it/s]\n[NeMo W 2026-01-25 15:58:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.72it/s]\n[NeMo W 2026-01-25 15:58:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.12it/s]\n[NeMo W 2026-01-25 15:58:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.71it/s]\n[NeMo W 2026-01-25 15:58:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.64it/s]\n[NeMo W 2026-01-25 15:58:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.13it/s]\n[NeMo W 2026-01-25 15:58:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.35it/s]\n[NeMo W 2026-01-25 15:58:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.43it/s]\n[NeMo W 2026-01-25 15:58:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.56it/s]\n[NeMo W 2026-01-25 15:58:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.82it/s]\n[NeMo W 2026-01-25 15:58:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.41it/s]\n[NeMo W 2026-01-25 15:58:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.64it/s]\n[NeMo W 2026-01-25 15:58:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.78it/s]\n[NeMo W 2026-01-25 15:58:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.24it/s]\n[NeMo W 2026-01-25 15:58:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.72it/s]\n[NeMo W 2026-01-25 15:58:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.00it/s]\n[NeMo W 2026-01-25 15:58:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.07it/s]\n[NeMo W 2026-01-25 15:58:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.85it/s]\n[NeMo W 2026-01-25 15:58:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.43it/s]\n[NeMo W 2026-01-25 15:58:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.75it/s]\n[NeMo W 2026-01-25 15:58:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.57it/s]\n[NeMo W 2026-01-25 15:58:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.25it/s]\n[NeMo W 2026-01-25 15:58:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.43it/s]\n[NeMo W 2026-01-25 15:58:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.44it/s]\n[NeMo W 2026-01-25 15:58:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.14it/s]\n[NeMo W 2026-01-25 15:58:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.31it/s]\n[NeMo W 2026-01-25 15:58:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.02it/s]\n[NeMo W 2026-01-25 15:58:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.82it/s]\n[NeMo W 2026-01-25 15:58:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.29it/s]\n[NeMo W 2026-01-25 15:58:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.95it/s]\n[NeMo W 2026-01-25 15:58:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 15:58:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.40it/s]\n[NeMo W 2026-01-25 15:58:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.05it/s]\n[NeMo W 2026-01-25 15:58:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.01it/s]\n[NeMo W 2026-01-25 15:58:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.63it/s]\n[NeMo W 2026-01-25 15:58:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.77it/s]\n[NeMo W 2026-01-25 15:58:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.04it/s]\n[NeMo W 2026-01-25 15:58:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.98it/s]\n[NeMo W 2026-01-25 15:58:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.84it/s]\n[NeMo W 2026-01-25 15:58:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:58:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.42it/s]\n[NeMo W 2026-01-25 15:59:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.46it/s]\n[NeMo W 2026-01-25 15:59:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.74it/s]\n[NeMo W 2026-01-25 15:59:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.58it/s]\n[NeMo W 2026-01-25 15:59:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.11it/s]\n[NeMo W 2026-01-25 15:59:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.72it/s]\n[NeMo W 2026-01-25 15:59:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.31it/s]\n[NeMo W 2026-01-25 15:59:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.88it/s]\n[NeMo W 2026-01-25 15:59:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.82it/s]\n[NeMo W 2026-01-25 15:59:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.86it/s]\n[NeMo W 2026-01-25 15:59:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.05it/s]\n[NeMo W 2026-01-25 15:59:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.12it/s]\n[NeMo W 2026-01-25 15:59:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.84it/s]\n[NeMo W 2026-01-25 15:59:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.13it/s]\n[NeMo W 2026-01-25 15:59:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.97it/s]\n[NeMo W 2026-01-25 15:59:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.89it/s]\n[NeMo W 2026-01-25 15:59:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.39it/s]\n[NeMo W 2026-01-25 15:59:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.66it/s]\n[NeMo W 2026-01-25 15:59:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.34it/s]\n[NeMo W 2026-01-25 15:59:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.59it/s]\n[NeMo W 2026-01-25 15:59:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.49it/s]\n[NeMo W 2026-01-25 15:59:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.56it/s]\n[NeMo W 2026-01-25 15:59:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.33it/s]\n[NeMo W 2026-01-25 15:59:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.83it/s]\n[NeMo W 2026-01-25 15:59:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.95it/s]\n[NeMo W 2026-01-25 15:59:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.72it/s]\n[NeMo W 2026-01-25 15:59:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.02it/s]\n[NeMo W 2026-01-25 15:59:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.84it/s]\n[NeMo W 2026-01-25 15:59:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.04it/s]\n[NeMo W 2026-01-25 15:59:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.56it/s]\n[NeMo W 2026-01-25 15:59:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.75it/s]\n[NeMo W 2026-01-25 15:59:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 15:59:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.48it/s]\n[NeMo W 2026-01-25 15:59:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.07it/s]\n[NeMo W 2026-01-25 15:59:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.99it/s]\n[NeMo W 2026-01-25 15:59:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.77it/s]\n[NeMo W 2026-01-25 15:59:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.48it/s]\n[NeMo W 2026-01-25 15:59:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.22it/s]\n[NeMo W 2026-01-25 15:59:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.15it/s]\n[NeMo W 2026-01-25 15:59:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.93it/s]\n[NeMo W 2026-01-25 15:59:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.52it/s]\n[NeMo W 2026-01-25 15:59:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.68it/s]\n[NeMo W 2026-01-25 15:59:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.57it/s]\n[NeMo W 2026-01-25 15:59:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.97it/s]\n[NeMo W 2026-01-25 15:59:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.32it/s]\n[NeMo W 2026-01-25 15:59:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 15:59:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.79it/s]\n[NeMo W 2026-01-25 15:59:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.08it/s]\n[NeMo W 2026-01-25 15:59:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.83it/s]\n[NeMo W 2026-01-25 15:59:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.02it/s]\n[NeMo W 2026-01-25 15:59:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.05it/s]\n[NeMo W 2026-01-25 15:59:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.20it/s]\n[NeMo W 2026-01-25 15:59:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.31it/s]\n[NeMo W 2026-01-25 15:59:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.80it/s]\n[NeMo W 2026-01-25 15:59:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.76it/s]\n[NeMo W 2026-01-25 15:59:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.92it/s]\n[NeMo W 2026-01-25 15:59:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:01,  1.04s/it]\n[NeMo W 2026-01-25 15:59:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.59it/s]\n[NeMo W 2026-01-25 15:59:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.27it/s]\n[NeMo W 2026-01-25 15:59:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.87it/s]\n[NeMo W 2026-01-25 15:59:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 15:59:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.33it/s]\n[NeMo W 2026-01-25 15:59:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.90it/s]\n[NeMo W 2026-01-25 15:59:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.00it/s]\n[NeMo W 2026-01-25 15:59:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.01it/s]\n[NeMo W 2026-01-25 15:59:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.84it/s]\n[NeMo W 2026-01-25 15:59:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.80it/s]\n[NeMo W 2026-01-25 15:59:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.57it/s]\n[NeMo W 2026-01-25 15:59:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.58it/s]\n[NeMo W 2026-01-25 15:59:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.77it/s]\n[NeMo W 2026-01-25 15:59:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.21it/s]\n[NeMo W 2026-01-25 15:59:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.52it/s]\n[NeMo W 2026-01-25 15:59:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.83it/s]\n[NeMo W 2026-01-25 15:59:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.10it/s]\n[NeMo W 2026-01-25 15:59:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.91it/s]\n[NeMo W 2026-01-25 15:59:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.51it/s]\n[NeMo W 2026-01-25 15:59:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.95it/s]\n[NeMo W 2026-01-25 15:59:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.23it/s]\n[NeMo W 2026-01-25 15:59:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.20it/s]\n[NeMo W 2026-01-25 15:59:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.95it/s]\n[NeMo W 2026-01-25 15:59:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.74it/s]\n[NeMo W 2026-01-25 15:59:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.51it/s]\n[NeMo W 2026-01-25 15:59:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.55it/s]\n[NeMo W 2026-01-25 15:59:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.54it/s]\n[NeMo W 2026-01-25 15:59:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.36it/s]\n[NeMo W 2026-01-25 15:59:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.57it/s]\n[NeMo W 2026-01-25 15:59:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.31it/s]\n[NeMo W 2026-01-25 15:59:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.64it/s]\n[NeMo W 2026-01-25 15:59:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.76it/s]\n[NeMo W 2026-01-25 15:59:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.31it/s]\n[NeMo W 2026-01-25 15:59:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 15:59:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.23it/s]\n[NeMo W 2026-01-25 15:59:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.36it/s]\n[NeMo W 2026-01-25 15:59:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.05it/s]\n[NeMo W 2026-01-25 15:59:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.88it/s]\n[NeMo W 2026-01-25 15:59:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.20it/s]\n[NeMo W 2026-01-25 15:59:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 15:59:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.02it/s]\n[NeMo W 2026-01-25 15:59:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.27it/s]\n[NeMo W 2026-01-25 15:59:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.50it/s]\n[NeMo W 2026-01-25 15:59:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.34it/s]\n[NeMo W 2026-01-25 15:59:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.69it/s]\n[NeMo W 2026-01-25 15:59:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.31it/s]\n[NeMo W 2026-01-25 15:59:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.20it/s]\n[NeMo W 2026-01-25 15:59:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.97it/s]\n[NeMo W 2026-01-25 15:59:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.68it/s]\n[NeMo W 2026-01-25 15:59:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.56it/s]\n[NeMo W 2026-01-25 15:59:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.13it/s]\n[NeMo W 2026-01-25 15:59:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.03it/s]\n[NeMo W 2026-01-25 15:59:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.68it/s]\n[NeMo W 2026-01-25 15:59:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.09it/s]\n[NeMo W 2026-01-25 15:59:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.80it/s]\n[NeMo W 2026-01-25 15:59:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.32it/s]\n[NeMo W 2026-01-25 15:59:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 15:59:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.76it/s]\n[NeMo W 2026-01-25 15:59:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.55it/s]\n[NeMo W 2026-01-25 15:59:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.00it/s]\n[NeMo W 2026-01-25 15:59:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.88it/s]\n[NeMo W 2026-01-25 15:59:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.66it/s]\n[NeMo W 2026-01-25 15:59:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.63it/s]\n[NeMo W 2026-01-25 15:59:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.79it/s]\n[NeMo W 2026-01-25 15:59:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.64it/s]\n[NeMo W 2026-01-25 15:59:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.65it/s]\n[NeMo W 2026-01-25 15:59:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.12it/s]\n[NeMo W 2026-01-25 15:59:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.77it/s]\n[NeMo W 2026-01-25 15:59:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.14it/s]\n[NeMo W 2026-01-25 15:59:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.16it/s]\n[NeMo W 2026-01-25 15:59:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.37it/s]\n[NeMo W 2026-01-25 15:59:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.20it/s]\n[NeMo W 2026-01-25 15:59:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.84it/s]\n[NeMo W 2026-01-25 15:59:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.86it/s]\n[NeMo W 2026-01-25 15:59:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.09it/s]\n[NeMo W 2026-01-25 15:59:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.77it/s]\n[NeMo W 2026-01-25 15:59:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.42it/s]\n[NeMo W 2026-01-25 15:59:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.09it/s]\n[NeMo W 2026-01-25 15:59:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.54it/s]\n[NeMo W 2026-01-25 15:59:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.83it/s]\n[NeMo W 2026-01-25 15:59:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.54it/s]\n[NeMo W 2026-01-25 15:59:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.85it/s]\n[NeMo W 2026-01-25 15:59:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.62it/s]\n[NeMo W 2026-01-25 15:59:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.85it/s]\n[NeMo W 2026-01-25 15:59:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.67it/s]\n[NeMo W 2026-01-25 15:59:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.54it/s]\n[NeMo W 2026-01-25 15:59:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.79it/s]\n[NeMo W 2026-01-25 15:59:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.94it/s]\n[NeMo W 2026-01-25 15:59:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.41it/s]\n[NeMo W 2026-01-25 15:59:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.07it/s]\n[NeMo W 2026-01-25 15:59:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.84it/s]\n[NeMo W 2026-01-25 15:59:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.73it/s]\n[NeMo W 2026-01-25 15:59:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.51it/s]\n[NeMo W 2026-01-25 15:59:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.72it/s]\n[NeMo W 2026-01-25 15:59:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.10it/s]\n[NeMo W 2026-01-25 15:59:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.51it/s]\n[NeMo W 2026-01-25 15:59:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.35it/s]\n[NeMo W 2026-01-25 15:59:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.46it/s]\n[NeMo W 2026-01-25 15:59:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.24it/s]\n[NeMo W 2026-01-25 15:59:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.89it/s]\n[NeMo W 2026-01-25 15:59:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.06it/s]\n[NeMo W 2026-01-25 15:59:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.26it/s]\n[NeMo W 2026-01-25 15:59:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.09it/s]\n[NeMo W 2026-01-25 15:59:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.04it/s]\n[NeMo W 2026-01-25 15:59:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.16it/s]\n[NeMo W 2026-01-25 15:59:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.48it/s]\n[NeMo W 2026-01-25 15:59:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.95it/s]\n[NeMo W 2026-01-25 15:59:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 15:59:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.13it/s]\n[NeMo W 2026-01-25 16:00:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.85it/s]\n[NeMo W 2026-01-25 16:00:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.23it/s]\n[NeMo W 2026-01-25 16:00:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.92it/s]\n[NeMo W 2026-01-25 16:00:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.14it/s]\n[NeMo W 2026-01-25 16:00:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.71it/s]\n[NeMo W 2026-01-25 16:00:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.92it/s]\n[NeMo W 2026-01-25 16:00:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.89it/s]\n[NeMo W 2026-01-25 16:00:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.36it/s]\n[NeMo W 2026-01-25 16:00:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.28it/s]\n[NeMo W 2026-01-25 16:00:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.84it/s]\n[NeMo W 2026-01-25 16:00:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.79it/s]\n[NeMo W 2026-01-25 16:00:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.45it/s]\n[NeMo W 2026-01-25 16:00:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.39it/s]\n[NeMo W 2026-01-25 16:00:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.40it/s]\n[NeMo W 2026-01-25 16:00:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.78it/s]\n[NeMo W 2026-01-25 16:00:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.13it/s]\n[NeMo W 2026-01-25 16:00:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.09it/s]\n[NeMo W 2026-01-25 16:00:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.03it/s]\n[NeMo W 2026-01-25 16:00:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.74it/s]\n[NeMo W 2026-01-25 16:00:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.50it/s]\n[NeMo W 2026-01-25 16:00:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.08it/s]\n[NeMo W 2026-01-25 16:00:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.36it/s]\n[NeMo W 2026-01-25 16:00:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.46it/s]\n[NeMo W 2026-01-25 16:00:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.32it/s]\n[NeMo W 2026-01-25 16:00:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.72it/s]\n[NeMo W 2026-01-25 16:00:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.88it/s]\n[NeMo W 2026-01-25 16:00:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.08it/s]\n[NeMo W 2026-01-25 16:00:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.41it/s]\n[NeMo W 2026-01-25 16:00:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.82it/s]\n[NeMo W 2026-01-25 16:00:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.51it/s]\n[NeMo W 2026-01-25 16:00:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.79it/s]\n[NeMo W 2026-01-25 16:00:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.79it/s]\n[NeMo W 2026-01-25 16:00:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.28it/s]\n[NeMo W 2026-01-25 16:00:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.70it/s]\n[NeMo W 2026-01-25 16:00:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.37it/s]\n[NeMo W 2026-01-25 16:00:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.80it/s]\n[NeMo W 2026-01-25 16:00:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.46it/s]\n[NeMo W 2026-01-25 16:00:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.21it/s]\n[NeMo W 2026-01-25 16:00:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.97it/s]\n[NeMo W 2026-01-25 16:00:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.86it/s]\n[NeMo W 2026-01-25 16:00:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.80it/s]\n[NeMo W 2026-01-25 16:00:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.69it/s]\n[NeMo W 2026-01-25 16:00:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.44it/s]\n[NeMo W 2026-01-25 16:00:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.51it/s]\n[NeMo W 2026-01-25 16:00:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.38it/s]\n[NeMo W 2026-01-25 16:00:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.21it/s]\n[NeMo W 2026-01-25 16:00:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.26it/s]\n[NeMo W 2026-01-25 16:00:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.82it/s]\n[NeMo W 2026-01-25 16:00:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.52it/s]\n[NeMo W 2026-01-25 16:00:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.77it/s]\n[NeMo W 2026-01-25 16:00:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.89it/s]\n[NeMo W 2026-01-25 16:00:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.12it/s]\n[NeMo W 2026-01-25 16:00:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.37it/s]\n[NeMo W 2026-01-25 16:00:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.24it/s]\n[NeMo W 2026-01-25 16:00:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.08it/s]\n[NeMo W 2026-01-25 16:00:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.57it/s]\n[NeMo W 2026-01-25 16:00:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.71it/s]\n[NeMo W 2026-01-25 16:00:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.62it/s]\n[NeMo W 2026-01-25 16:00:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.43it/s]\n[NeMo W 2026-01-25 16:00:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.68it/s]\n[NeMo W 2026-01-25 16:00:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.01it/s]\n[NeMo W 2026-01-25 16:00:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.09it/s]\n[NeMo W 2026-01-25 16:00:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.87it/s]\n[NeMo W 2026-01-25 16:00:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.54it/s]\n[NeMo W 2026-01-25 16:00:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.57it/s]\n[NeMo W 2026-01-25 16:00:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.10it/s]\n[NeMo W 2026-01-25 16:00:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.16it/s]\n[NeMo W 2026-01-25 16:00:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.70it/s]\n[NeMo W 2026-01-25 16:00:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.38it/s]\n[NeMo W 2026-01-25 16:00:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.68it/s]\n[NeMo W 2026-01-25 16:00:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.30it/s]\n[NeMo W 2026-01-25 16:00:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.43it/s]\n[NeMo W 2026-01-25 16:00:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.77it/s]\n[NeMo W 2026-01-25 16:00:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.99it/s]\n[NeMo W 2026-01-25 16:00:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 16:00:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.75it/s]\n[NeMo W 2026-01-25 16:00:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.19it/s]\n[NeMo W 2026-01-25 16:00:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.98it/s]\n[NeMo W 2026-01-25 16:00:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.08it/s]\n[NeMo W 2026-01-25 16:00:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.39it/s]\n[NeMo W 2026-01-25 16:00:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 16:00:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.17it/s]\n[NeMo W 2026-01-25 16:00:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.19it/s]\n[NeMo W 2026-01-25 16:00:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.08it/s]\n[NeMo W 2026-01-25 16:00:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.00it/s]\n[NeMo W 2026-01-25 16:00:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.56it/s]\n[NeMo W 2026-01-25 16:00:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.45it/s]\n[NeMo W 2026-01-25 16:00:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.77it/s]\n[NeMo W 2026-01-25 16:00:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.81it/s]\n[NeMo W 2026-01-25 16:00:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.50it/s]\n[NeMo W 2026-01-25 16:00:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.49it/s]\n[NeMo W 2026-01-25 16:00:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.84it/s]\n[NeMo W 2026-01-25 16:00:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.38it/s]\n[NeMo W 2026-01-25 16:00:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.58it/s]\n[NeMo W 2026-01-25 16:00:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.56it/s]\n[NeMo W 2026-01-25 16:00:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.47it/s]\n[NeMo W 2026-01-25 16:00:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 16:00:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.55it/s]\n[NeMo W 2026-01-25 16:00:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.75it/s]\n[NeMo W 2026-01-25 16:00:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.95it/s]\n[NeMo W 2026-01-25 16:00:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.03it/s]\n[NeMo W 2026-01-25 16:00:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.82it/s]\n[NeMo W 2026-01-25 16:00:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.07it/s]\n[NeMo W 2026-01-25 16:00:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.23it/s]\n[NeMo W 2026-01-25 16:00:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.32it/s]\n[NeMo W 2026-01-25 16:00:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.07it/s]\n[NeMo W 2026-01-25 16:00:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.45it/s]\n[NeMo W 2026-01-25 16:00:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.76it/s]\n[NeMo W 2026-01-25 16:00:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.48it/s]\n[NeMo W 2026-01-25 16:00:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.24it/s]\n[NeMo W 2026-01-25 16:00:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.90it/s]\n[NeMo W 2026-01-25 16:00:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.60it/s]\n[NeMo W 2026-01-25 16:00:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.43it/s]\n[NeMo W 2026-01-25 16:00:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.72it/s]\n[NeMo W 2026-01-25 16:00:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.60it/s]\n[NeMo W 2026-01-25 16:00:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.40it/s]\n[NeMo W 2026-01-25 16:00:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.19it/s]\n[NeMo W 2026-01-25 16:00:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.17it/s]\n[NeMo W 2026-01-25 16:00:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.62it/s]\n[NeMo W 2026-01-25 16:00:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.39it/s]\n[NeMo W 2026-01-25 16:00:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.48it/s]\n[NeMo W 2026-01-25 16:00:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.72it/s]\n[NeMo W 2026-01-25 16:00:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.59it/s]\n[NeMo W 2026-01-25 16:00:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.89it/s]\n[NeMo W 2026-01-25 16:00:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.70it/s]\n[NeMo W 2026-01-25 16:00:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.13it/s]\n[NeMo W 2026-01-25 16:00:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.50it/s]\n[NeMo W 2026-01-25 16:00:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.09it/s]\n[NeMo W 2026-01-25 16:00:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.34it/s]\n[NeMo W 2026-01-25 16:00:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.79it/s]\n[NeMo W 2026-01-25 16:00:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.60it/s]\n[NeMo W 2026-01-25 16:00:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.45it/s]\n[NeMo W 2026-01-25 16:00:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.02it/s]\n[NeMo W 2026-01-25 16:00:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.47it/s]\n[NeMo W 2026-01-25 16:00:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.52it/s]\n[NeMo W 2026-01-25 16:00:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.57it/s]\n[NeMo W 2026-01-25 16:00:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.44it/s]\n[NeMo W 2026-01-25 16:00:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.75it/s]\n[NeMo W 2026-01-25 16:00:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.99it/s]\n[NeMo W 2026-01-25 16:00:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.02it/s]\n[NeMo W 2026-01-25 16:00:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.63it/s]\n[NeMo W 2026-01-25 16:00:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.42it/s]\n[NeMo W 2026-01-25 16:00:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.88it/s]\n[NeMo W 2026-01-25 16:00:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.53it/s]\n[NeMo W 2026-01-25 16:00:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.24it/s]\n[NeMo W 2026-01-25 16:00:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.14it/s]\n[NeMo W 2026-01-25 16:00:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.32it/s]\n[NeMo W 2026-01-25 16:00:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.07it/s]\n[NeMo W 2026-01-25 16:00:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.12it/s]\n[NeMo W 2026-01-25 16:00:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.64it/s]\n[NeMo W 2026-01-25 16:00:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.82it/s]\n[NeMo W 2026-01-25 16:00:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.90it/s]\n[NeMo W 2026-01-25 16:00:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.23it/s]\n[NeMo W 2026-01-25 16:00:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.34it/s]\n[NeMo W 2026-01-25 16:00:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.26it/s]\n[NeMo W 2026-01-25 16:00:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.34it/s]\n[NeMo W 2026-01-25 16:00:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.37it/s]\n[NeMo W 2026-01-25 16:00:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.92it/s]\n[NeMo W 2026-01-25 16:00:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.65it/s]\n[NeMo W 2026-01-25 16:00:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.19it/s]\n[NeMo W 2026-01-25 16:00:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.74it/s]\n[NeMo W 2026-01-25 16:00:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.22it/s]\n[NeMo W 2026-01-25 16:00:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.50it/s]\n[NeMo W 2026-01-25 16:00:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.16it/s]\n[NeMo W 2026-01-25 16:00:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.17it/s]\n[NeMo W 2026-01-25 16:00:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:00:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.34it/s]\n[NeMo W 2026-01-25 16:01:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.91it/s]\n[NeMo W 2026-01-25 16:01:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.82it/s]\n[NeMo W 2026-01-25 16:01:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.67it/s]\n[NeMo W 2026-01-25 16:01:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.62it/s]\n[NeMo W 2026-01-25 16:01:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.77it/s]\n[NeMo W 2026-01-25 16:01:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.77it/s]\n[NeMo W 2026-01-25 16:01:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.95it/s]\n[NeMo W 2026-01-25 16:01:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.57it/s]\n[NeMo W 2026-01-25 16:01:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.08it/s]\n[NeMo W 2026-01-25 16:01:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.22it/s]\n[NeMo W 2026-01-25 16:01:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.24it/s]\n[NeMo W 2026-01-25 16:01:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.30it/s]\n[NeMo W 2026-01-25 16:01:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.83it/s]\n[NeMo W 2026-01-25 16:01:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.24it/s]\n[NeMo W 2026-01-25 16:01:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.85it/s]\n[NeMo W 2026-01-25 16:01:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.78it/s]\n[NeMo W 2026-01-25 16:01:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.06it/s]\n[NeMo W 2026-01-25 16:01:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.44it/s]\n[NeMo W 2026-01-25 16:01:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.73it/s]\n[NeMo W 2026-01-25 16:01:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.52it/s]\n[NeMo W 2026-01-25 16:01:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.54it/s]\n[NeMo W 2026-01-25 16:01:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.09it/s]\n[NeMo W 2026-01-25 16:01:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.11it/s]\n[NeMo W 2026-01-25 16:01:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.11it/s]\n[NeMo W 2026-01-25 16:01:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.91it/s]\n[NeMo W 2026-01-25 16:01:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.42it/s]\n[NeMo W 2026-01-25 16:01:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.84it/s]\n[NeMo W 2026-01-25 16:01:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.31it/s]\n[NeMo W 2026-01-25 16:01:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.83it/s]\n[NeMo W 2026-01-25 16:01:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.02it/s]\n[NeMo W 2026-01-25 16:01:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.34it/s]\n[NeMo W 2026-01-25 16:01:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.90it/s]\n[NeMo W 2026-01-25 16:01:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.95it/s]\n[NeMo W 2026-01-25 16:01:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.11it/s]\n[NeMo W 2026-01-25 16:01:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.05it/s]\n[NeMo W 2026-01-25 16:01:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 16:01:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.94it/s]\n[NeMo W 2026-01-25 16:01:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.64it/s]\n[NeMo W 2026-01-25 16:01:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.44it/s]\n[NeMo W 2026-01-25 16:01:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.62it/s]\n[NeMo W 2026-01-25 16:01:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.20it/s]\n[NeMo W 2026-01-25 16:01:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.41it/s]\n[NeMo W 2026-01-25 16:01:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.61it/s]\n[NeMo W 2026-01-25 16:01:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.32it/s]\n[NeMo W 2026-01-25 16:01:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.18it/s]\n[NeMo W 2026-01-25 16:01:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.72it/s]\n[NeMo W 2026-01-25 16:01:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.17it/s]\n[NeMo W 2026-01-25 16:01:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.41it/s]\n[NeMo W 2026-01-25 16:01:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.48it/s]\n[NeMo W 2026-01-25 16:01:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.24it/s]\n[NeMo W 2026-01-25 16:01:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.63it/s]\n[NeMo W 2026-01-25 16:01:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.57it/s]\n[NeMo W 2026-01-25 16:01:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.88it/s]\n[NeMo W 2026-01-25 16:01:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.63it/s]\n[NeMo W 2026-01-25 16:01:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.13it/s]\n[NeMo W 2026-01-25 16:01:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.94it/s]\n[NeMo W 2026-01-25 16:01:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.01it/s]\n[NeMo W 2026-01-25 16:01:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.72it/s]\n[NeMo W 2026-01-25 16:01:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.55it/s]\n[NeMo W 2026-01-25 16:01:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.76it/s]\n[NeMo W 2026-01-25 16:01:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.23it/s]\n[NeMo W 2026-01-25 16:01:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.43it/s]\n[NeMo W 2026-01-25 16:01:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.26it/s]\n[NeMo W 2026-01-25 16:01:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.47it/s]\n[NeMo W 2026-01-25 16:01:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.90it/s]\n[NeMo W 2026-01-25 16:01:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.87it/s]\n[NeMo W 2026-01-25 16:01:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.08it/s]\n[NeMo W 2026-01-25 16:01:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.21it/s]\n[NeMo W 2026-01-25 16:01:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.76it/s]\n[NeMo W 2026-01-25 16:01:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.59it/s]\n[NeMo W 2026-01-25 16:01:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.59it/s]\n[NeMo W 2026-01-25 16:01:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.73it/s]\n[NeMo W 2026-01-25 16:01:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.37it/s]\n[NeMo W 2026-01-25 16:01:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.29it/s]\n[NeMo W 2026-01-25 16:01:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.20it/s]\n[NeMo W 2026-01-25 16:01:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 16:01:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.71it/s]\n[NeMo W 2026-01-25 16:01:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.75it/s]\n[NeMo W 2026-01-25 16:01:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.83it/s]\n[NeMo W 2026-01-25 16:01:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.33it/s]\n[NeMo W 2026-01-25 16:01:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.57it/s]\n[NeMo W 2026-01-25 16:01:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.83it/s]\n[NeMo W 2026-01-25 16:01:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.03it/s]\n[NeMo W 2026-01-25 16:01:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.41it/s]\n[NeMo W 2026-01-25 16:01:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.22it/s]\n[NeMo W 2026-01-25 16:01:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.53it/s]\n[NeMo W 2026-01-25 16:01:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.01it/s]\n[NeMo W 2026-01-25 16:01:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.18it/s]\n[NeMo W 2026-01-25 16:01:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.95it/s]\n[NeMo W 2026-01-25 16:01:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.18it/s]\n[NeMo W 2026-01-25 16:01:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.17it/s]\n[NeMo W 2026-01-25 16:01:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.87it/s]\n[NeMo W 2026-01-25 16:01:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.92it/s]\n[NeMo W 2026-01-25 16:01:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.30it/s]\n[NeMo W 2026-01-25 16:01:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.74it/s]\n[NeMo W 2026-01-25 16:01:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.69it/s]\n[NeMo W 2026-01-25 16:01:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.05it/s]\n[NeMo W 2026-01-25 16:01:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.72it/s]\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.28it/s]\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.84it/s]\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.74it/s]\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.93it/s]\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.22it/s]\n[NeMo W 2026-01-25 16:01:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.37it/s]\n[NeMo W 2026-01-25 16:01:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.15it/s]\n[NeMo W 2026-01-25 16:01:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.69it/s]\n[NeMo W 2026-01-25 16:01:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.08it/s]\n[NeMo W 2026-01-25 16:01:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.88it/s]\n[NeMo W 2026-01-25 16:01:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.90it/s]\n[NeMo W 2026-01-25 16:01:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.49it/s]\n[NeMo W 2026-01-25 16:01:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.50it/s]\n[NeMo W 2026-01-25 16:01:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.43it/s]\n[NeMo W 2026-01-25 16:01:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.23it/s]\n[NeMo W 2026-01-25 16:01:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.08it/s]\n[NeMo W 2026-01-25 16:01:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.26it/s]\n[NeMo W 2026-01-25 16:01:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.65it/s]\n[NeMo W 2026-01-25 16:01:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.68it/s]\n[NeMo W 2026-01-25 16:01:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.40it/s]\n[NeMo W 2026-01-25 16:01:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.17it/s]\n[NeMo W 2026-01-25 16:01:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.47it/s]\n[NeMo W 2026-01-25 16:01:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.86it/s]\n[NeMo W 2026-01-25 16:01:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.44it/s]\n[NeMo W 2026-01-25 16:01:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.22it/s]\n[NeMo W 2026-01-25 16:01:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.42it/s]\n[NeMo W 2026-01-25 16:01:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.60it/s]\n[NeMo W 2026-01-25 16:01:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.91it/s]\n[NeMo W 2026-01-25 16:01:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.06it/s]\n[NeMo W 2026-01-25 16:01:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.96it/s]\n[NeMo W 2026-01-25 16:01:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.31it/s]\n[NeMo W 2026-01-25 16:01:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.15it/s]\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.25it/s]\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.62it/s]\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.34it/s]\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.09it/s]\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.94it/s]\n[NeMo W 2026-01-25 16:01:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.47it/s]\n[NeMo W 2026-01-25 16:01:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.93it/s]\n[NeMo W 2026-01-25 16:01:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.46it/s]\n[NeMo W 2026-01-25 16:01:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.64it/s]\n[NeMo W 2026-01-25 16:01:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.91it/s]\n[NeMo W 2026-01-25 16:01:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.80it/s]\n[NeMo W 2026-01-25 16:01:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.20it/s]\n[NeMo W 2026-01-25 16:01:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.88it/s]\n[NeMo W 2026-01-25 16:01:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.39it/s]\n[NeMo W 2026-01-25 16:01:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.20it/s]\n[NeMo W 2026-01-25 16:01:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.03it/s]\n[NeMo W 2026-01-25 16:01:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.13it/s]\n[NeMo W 2026-01-25 16:01:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.30it/s]\n[NeMo W 2026-01-25 16:01:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.55it/s]\n[NeMo W 2026-01-25 16:01:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.77it/s]\n[NeMo W 2026-01-25 16:01:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.30it/s]\n[NeMo W 2026-01-25 16:01:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.89it/s]\n[NeMo W 2026-01-25 16:01:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.38it/s]\n[NeMo W 2026-01-25 16:01:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.25it/s]\n[NeMo W 2026-01-25 16:01:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.42it/s]\n[NeMo W 2026-01-25 16:01:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00, 10.41it/s]\n[NeMo W 2026-01-25 16:01:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.67it/s]\n[NeMo W 2026-01-25 16:01:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.86it/s]\n[NeMo W 2026-01-25 16:01:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.00it/s]\n[NeMo W 2026-01-25 16:01:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.79it/s]\n[NeMo W 2026-01-25 16:01:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.24it/s]\n[NeMo W 2026-01-25 16:01:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.53it/s]\n[NeMo W 2026-01-25 16:01:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.77it/s]\n[NeMo W 2026-01-25 16:01:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.97it/s]\n[NeMo W 2026-01-25 16:01:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.19it/s]\n[NeMo W 2026-01-25 16:01:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.31it/s]\n[NeMo W 2026-01-25 16:01:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.38it/s]\n[NeMo W 2026-01-25 16:01:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.38it/s]\n[NeMo W 2026-01-25 16:01:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.46it/s]\n[NeMo W 2026-01-25 16:01:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.99it/s]\n[NeMo W 2026-01-25 16:01:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:01:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.09it/s]\n[NeMo W 2026-01-25 16:02:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.22it/s]\n[NeMo W 2026-01-25 16:02:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.67it/s]\n[NeMo W 2026-01-25 16:02:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.36it/s]\n[NeMo W 2026-01-25 16:02:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.00it/s]\n[NeMo W 2026-01-25 16:02:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.00it/s]\n[NeMo W 2026-01-25 16:02:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.43it/s]\n[NeMo W 2026-01-25 16:02:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.99it/s]\n[NeMo W 2026-01-25 16:02:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.55it/s]\n[NeMo W 2026-01-25 16:02:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.38it/s]\n[NeMo W 2026-01-25 16:02:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.95it/s]\n[NeMo W 2026-01-25 16:02:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.32it/s]\n[NeMo W 2026-01-25 16:02:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.46it/s]\n[NeMo W 2026-01-25 16:02:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.15it/s]\n[NeMo W 2026-01-25 16:02:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.00it/s]\n[NeMo W 2026-01-25 16:02:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.89it/s]\n[NeMo W 2026-01-25 16:02:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.71it/s]\n[NeMo W 2026-01-25 16:02:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.86it/s]\n[NeMo W 2026-01-25 16:02:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.28it/s]\n[NeMo W 2026-01-25 16:02:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.02it/s]\n[NeMo W 2026-01-25 16:02:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.59it/s]\n[NeMo W 2026-01-25 16:02:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.24it/s]\n[NeMo W 2026-01-25 16:02:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.60it/s]\n[NeMo W 2026-01-25 16:02:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.68it/s]\n[NeMo W 2026-01-25 16:02:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.00it/s]\n[NeMo W 2026-01-25 16:02:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.12it/s]\n[NeMo W 2026-01-25 16:02:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.72it/s]\n[NeMo W 2026-01-25 16:02:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.40it/s]\n[NeMo W 2026-01-25 16:02:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.63it/s]\n[NeMo W 2026-01-25 16:02:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.35it/s]\n[NeMo W 2026-01-25 16:02:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.59it/s]\n[NeMo W 2026-01-25 16:02:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.55it/s]\n[NeMo W 2026-01-25 16:02:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.14it/s]\n[NeMo W 2026-01-25 16:02:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.45it/s]\n[NeMo W 2026-01-25 16:02:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.80it/s]\n[NeMo W 2026-01-25 16:02:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.00it/s]\n[NeMo W 2026-01-25 16:02:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.85it/s]\n[NeMo W 2026-01-25 16:02:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.94it/s]\n[NeMo W 2026-01-25 16:02:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.56it/s]\n[NeMo W 2026-01-25 16:02:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.70it/s]\n[NeMo W 2026-01-25 16:02:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.46it/s]\n[NeMo W 2026-01-25 16:02:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.46it/s]\n[NeMo W 2026-01-25 16:02:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.63it/s]\n[NeMo W 2026-01-25 16:02:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.14it/s]\n[NeMo W 2026-01-25 16:02:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.79it/s]\n[NeMo W 2026-01-25 16:02:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.60it/s]\n[NeMo W 2026-01-25 16:02:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.11it/s]\n[NeMo W 2026-01-25 16:02:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.72it/s]\n[NeMo W 2026-01-25 16:02:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.23it/s]\n[NeMo W 2026-01-25 16:02:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.07it/s]\n[NeMo W 2026-01-25 16:02:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.64it/s]\n[NeMo W 2026-01-25 16:02:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.16it/s]\n[NeMo W 2026-01-25 16:02:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.94it/s]\n[NeMo W 2026-01-25 16:02:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.72it/s]\n[NeMo W 2026-01-25 16:02:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.80it/s]\n[NeMo W 2026-01-25 16:02:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.92it/s]\n[NeMo W 2026-01-25 16:02:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.01it/s]\n[NeMo W 2026-01-25 16:02:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.14it/s]\n[NeMo W 2026-01-25 16:02:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.70it/s]\n[NeMo W 2026-01-25 16:02:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.35it/s]\n[NeMo W 2026-01-25 16:02:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.35it/s]\n[NeMo W 2026-01-25 16:02:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.23it/s]\n[NeMo W 2026-01-25 16:02:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.28it/s]\n[NeMo W 2026-01-25 16:02:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.74it/s]\n[NeMo W 2026-01-25 16:02:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.37it/s]\n[NeMo W 2026-01-25 16:02:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.36it/s]\n[NeMo W 2026-01-25 16:02:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.69it/s]\n[NeMo W 2026-01-25 16:02:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.67it/s]\n[NeMo W 2026-01-25 16:02:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.01it/s]\n[NeMo W 2026-01-25 16:02:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.41it/s]\n[NeMo W 2026-01-25 16:02:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.01it/s]\n[NeMo W 2026-01-25 16:02:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.98it/s]\n[NeMo W 2026-01-25 16:02:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.16it/s]\n[NeMo W 2026-01-25 16:02:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.77it/s]\n[NeMo W 2026-01-25 16:02:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.13it/s]\n[NeMo W 2026-01-25 16:02:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.36it/s]\n[NeMo W 2026-01-25 16:02:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.27it/s]\n[NeMo W 2026-01-25 16:02:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.28it/s]\n[NeMo W 2026-01-25 16:02:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.71it/s]\n[NeMo W 2026-01-25 16:02:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.72it/s]\n[NeMo W 2026-01-25 16:02:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.89it/s]\n[NeMo W 2026-01-25 16:02:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.38it/s]\n[NeMo W 2026-01-25 16:02:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.48it/s]\n[NeMo W 2026-01-25 16:02:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.35it/s]\n[NeMo W 2026-01-25 16:02:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.32it/s]\n[NeMo W 2026-01-25 16:02:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.39it/s]\n[NeMo W 2026-01-25 16:02:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.50it/s]\n[NeMo W 2026-01-25 16:02:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.14it/s]\n[NeMo W 2026-01-25 16:02:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.47it/s]\n[NeMo W 2026-01-25 16:02:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.07it/s]\n[NeMo W 2026-01-25 16:02:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.63it/s]\n[NeMo W 2026-01-25 16:02:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.32it/s]\n[NeMo W 2026-01-25 16:02:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.18it/s]\n[NeMo W 2026-01-25 16:02:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.90it/s]\n[NeMo W 2026-01-25 16:02:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.15it/s]\n[NeMo W 2026-01-25 16:02:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.46it/s]\n[NeMo W 2026-01-25 16:02:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.82it/s]\n[NeMo W 2026-01-25 16:02:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.23it/s]\n[NeMo W 2026-01-25 16:02:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.68it/s]\n[NeMo W 2026-01-25 16:02:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.10it/s]\n[NeMo W 2026-01-25 16:02:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.73it/s]\n[NeMo W 2026-01-25 16:02:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.56it/s]\n[NeMo W 2026-01-25 16:02:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.29it/s]\n[NeMo W 2026-01-25 16:02:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.46it/s]\n[NeMo W 2026-01-25 16:02:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.68it/s]\n[NeMo W 2026-01-25 16:02:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.70it/s]\n[NeMo W 2026-01-25 16:02:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.53it/s]\n[NeMo W 2026-01-25 16:02:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.86it/s]\n[NeMo W 2026-01-25 16:02:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.71it/s]\n[NeMo W 2026-01-25 16:02:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.65it/s]\n[NeMo W 2026-01-25 16:02:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.90it/s]\n[NeMo W 2026-01-25 16:02:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.45it/s]\n[NeMo W 2026-01-25 16:02:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.10it/s]\n[NeMo W 2026-01-25 16:02:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.34it/s]\n[NeMo W 2026-01-25 16:02:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.40it/s]\n[NeMo W 2026-01-25 16:02:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.04it/s]\n[NeMo W 2026-01-25 16:02:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.32it/s]\n[NeMo W 2026-01-25 16:02:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.89it/s]\n[NeMo W 2026-01-25 16:02:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.67it/s]\n[NeMo W 2026-01-25 16:02:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.41it/s]\n[NeMo W 2026-01-25 16:02:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.09it/s]\n[NeMo W 2026-01-25 16:02:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.85it/s]\n[NeMo W 2026-01-25 16:02:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.74it/s]\n[NeMo W 2026-01-25 16:02:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.68it/s]\n[NeMo W 2026-01-25 16:02:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.44it/s]\n[NeMo W 2026-01-25 16:02:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.29it/s]\n[NeMo W 2026-01-25 16:02:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.74it/s]\n[NeMo W 2026-01-25 16:02:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.13it/s]\n[NeMo W 2026-01-25 16:02:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.85it/s]\n[NeMo W 2026-01-25 16:02:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.21it/s]\n[NeMo W 2026-01-25 16:02:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.05it/s]\n[NeMo W 2026-01-25 16:02:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.45it/s]\n[NeMo W 2026-01-25 16:02:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.94it/s]\n[NeMo W 2026-01-25 16:02:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.34it/s]\n[NeMo W 2026-01-25 16:02:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.28it/s]\n[NeMo W 2026-01-25 16:02:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.94it/s]\n[NeMo W 2026-01-25 16:02:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.04it/s]\n[NeMo W 2026-01-25 16:02:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.82it/s]\n[NeMo W 2026-01-25 16:02:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.18it/s]\n[NeMo W 2026-01-25 16:02:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.47it/s]\n[NeMo W 2026-01-25 16:02:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.50it/s]\n[NeMo W 2026-01-25 16:02:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.45it/s]\n[NeMo W 2026-01-25 16:02:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.19it/s]\n[NeMo W 2026-01-25 16:02:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 16:02:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.49it/s]\n[NeMo W 2026-01-25 16:02:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.03it/s]\n[NeMo W 2026-01-25 16:02:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.71it/s]\n[NeMo W 2026-01-25 16:02:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.26it/s]\n[NeMo W 2026-01-25 16:02:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.56it/s]\n[NeMo W 2026-01-25 16:02:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.83it/s]\n[NeMo W 2026-01-25 16:02:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.35it/s]\n[NeMo W 2026-01-25 16:02:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.34it/s]\n[NeMo W 2026-01-25 16:02:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.80it/s]\n[NeMo W 2026-01-25 16:02:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.35it/s]\n[NeMo W 2026-01-25 16:02:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.67it/s]\n[NeMo W 2026-01-25 16:02:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.72it/s]\n[NeMo W 2026-01-25 16:02:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.73it/s]\n[NeMo W 2026-01-25 16:02:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.52it/s]\n[NeMo W 2026-01-25 16:02:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.64it/s]\n[NeMo W 2026-01-25 16:02:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:02:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.88it/s]\n[NeMo W 2026-01-25 16:03:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.14it/s]\n[NeMo W 2026-01-25 16:03:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.87it/s]\n[NeMo W 2026-01-25 16:03:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  9.47it/s]\n[NeMo W 2026-01-25 16:03:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.92it/s]\n[NeMo W 2026-01-25 16:03:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.15it/s]\n[NeMo W 2026-01-25 16:03:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.15it/s]\n[NeMo W 2026-01-25 16:03:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.62it/s]\n[NeMo W 2026-01-25 16:03:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.25it/s]\n[NeMo W 2026-01-25 16:03:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.78it/s]\n[NeMo W 2026-01-25 16:03:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.73it/s]\n[NeMo W 2026-01-25 16:03:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.61it/s]\n[NeMo W 2026-01-25 16:03:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.22it/s]\n[NeMo W 2026-01-25 16:03:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.17it/s]\n[NeMo W 2026-01-25 16:03:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.74it/s]\n[NeMo W 2026-01-25 16:03:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.55it/s]\n[NeMo W 2026-01-25 16:03:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.87it/s]\n[NeMo W 2026-01-25 16:03:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.33it/s]\n[NeMo W 2026-01-25 16:03:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.77it/s]\n[NeMo W 2026-01-25 16:03:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.15it/s]\n[NeMo W 2026-01-25 16:03:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.76it/s]\n[NeMo W 2026-01-25 16:03:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.42it/s]\n[NeMo W 2026-01-25 16:03:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.63it/s]\n[NeMo W 2026-01-25 16:03:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.36it/s]\n[NeMo W 2026-01-25 16:03:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.55it/s]\n[NeMo W 2026-01-25 16:03:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.44it/s]\n[NeMo W 2026-01-25 16:03:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.23it/s]\n[NeMo W 2026-01-25 16:03:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.38it/s]\n[NeMo W 2026-01-25 16:03:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.21it/s]\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.53it/s]\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.38it/s]\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.84it/s]\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.91it/s]\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.19it/s]\n[NeMo W 2026-01-25 16:03:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.01it/s]\n[NeMo W 2026-01-25 16:03:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.13it/s]\n[NeMo W 2026-01-25 16:03:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.32it/s]\n[NeMo W 2026-01-25 16:03:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.41it/s]\n[NeMo W 2026-01-25 16:03:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.79it/s]\n[NeMo W 2026-01-25 16:03:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.58it/s]\n[NeMo W 2026-01-25 16:03:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.75it/s]\n[NeMo W 2026-01-25 16:03:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.24it/s]\n[NeMo W 2026-01-25 16:03:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.60it/s]\n[NeMo W 2026-01-25 16:03:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.10it/s]\n[NeMo W 2026-01-25 16:03:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.00it/s]\n[NeMo W 2026-01-25 16:03:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.58it/s]\n[NeMo W 2026-01-25 16:03:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.07it/s]\n[NeMo W 2026-01-25 16:03:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.14it/s]\n[NeMo W 2026-01-25 16:03:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.81it/s]\n[NeMo W 2026-01-25 16:03:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.66it/s]\n[NeMo W 2026-01-25 16:03:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.08it/s]\n[NeMo W 2026-01-25 16:03:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.94it/s]\n[NeMo W 2026-01-25 16:03:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.26it/s]\n[NeMo W 2026-01-25 16:03:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.56it/s]\n[NeMo W 2026-01-25 16:03:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.00it/s]\n[NeMo W 2026-01-25 16:03:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.11it/s]\n[NeMo W 2026-01-25 16:03:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.68it/s]\n[NeMo W 2026-01-25 16:03:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.89it/s]\n[NeMo W 2026-01-25 16:03:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.98it/s]\n[NeMo W 2026-01-25 16:03:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.23it/s]\n[NeMo W 2026-01-25 16:03:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.73it/s]\n[NeMo W 2026-01-25 16:03:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.79it/s]\n[NeMo W 2026-01-25 16:03:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.30it/s]\n[NeMo W 2026-01-25 16:03:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.08it/s]\n[NeMo W 2026-01-25 16:03:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.67it/s]\n[NeMo W 2026-01-25 16:03:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.34it/s]\n[NeMo W 2026-01-25 16:03:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.61it/s]\n[NeMo W 2026-01-25 16:03:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.85it/s]\n[NeMo W 2026-01-25 16:03:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.37it/s]\n[NeMo W 2026-01-25 16:03:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.71it/s]\n[NeMo W 2026-01-25 16:03:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.01it/s]\n[NeMo W 2026-01-25 16:03:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.19it/s]\n[NeMo W 2026-01-25 16:03:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.18it/s]\n[NeMo W 2026-01-25 16:03:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.44it/s]\n[NeMo W 2026-01-25 16:03:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.54it/s]\n[NeMo W 2026-01-25 16:03:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.17it/s]\n[NeMo W 2026-01-25 16:03:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.26it/s]\n[NeMo W 2026-01-25 16:03:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.99it/s]\n[NeMo W 2026-01-25 16:03:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.53it/s]\n[NeMo W 2026-01-25 16:03:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.65it/s]\n[NeMo W 2026-01-25 16:03:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.29it/s]\n[NeMo W 2026-01-25 16:03:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.34it/s]\n[NeMo W 2026-01-25 16:03:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.77it/s]\n[NeMo W 2026-01-25 16:03:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.07it/s]\n[NeMo W 2026-01-25 16:03:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.82it/s]\n[NeMo W 2026-01-25 16:03:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.90it/s]\n[NeMo W 2026-01-25 16:03:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.41it/s]\n[NeMo W 2026-01-25 16:03:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.89it/s]\n[NeMo W 2026-01-25 16:03:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.13it/s]\n[NeMo W 2026-01-25 16:03:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.88it/s]\n[NeMo W 2026-01-25 16:03:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.11it/s]\n[NeMo W 2026-01-25 16:03:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.36it/s]\n[NeMo W 2026-01-25 16:03:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.12it/s]\n[NeMo W 2026-01-25 16:03:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.11it/s]\n[NeMo W 2026-01-25 16:03:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.12it/s]\n[NeMo W 2026-01-25 16:03:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.11it/s]\n[NeMo W 2026-01-25 16:03:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.93it/s]\n[NeMo W 2026-01-25 16:03:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.74it/s]\n[NeMo W 2026-01-25 16:03:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.59it/s]\n[NeMo W 2026-01-25 16:03:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.02it/s]\n[NeMo W 2026-01-25 16:03:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.06it/s]\n[NeMo W 2026-01-25 16:03:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.18it/s]\n[NeMo W 2026-01-25 16:03:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.59it/s]\n[NeMo W 2026-01-25 16:03:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.91it/s]\n[NeMo W 2026-01-25 16:03:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.48it/s]\n[NeMo W 2026-01-25 16:03:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.51it/s]\n[NeMo W 2026-01-25 16:03:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.52it/s]\n[NeMo W 2026-01-25 16:03:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.66it/s]\n[NeMo W 2026-01-25 16:03:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.27it/s]\n[NeMo W 2026-01-25 16:03:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.18it/s]\n[NeMo W 2026-01-25 16:03:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.39it/s]\n[NeMo W 2026-01-25 16:03:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.25it/s]\n[NeMo W 2026-01-25 16:03:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.95it/s]\n[NeMo W 2026-01-25 16:03:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.98it/s]\n[NeMo W 2026-01-25 16:03:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.50it/s]\n[NeMo W 2026-01-25 16:03:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.78it/s]\n[NeMo W 2026-01-25 16:03:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.32it/s]\n[NeMo W 2026-01-25 16:03:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.97it/s]\n[NeMo W 2026-01-25 16:03:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.83it/s]\n[NeMo W 2026-01-25 16:03:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.67it/s]\n[NeMo W 2026-01-25 16:03:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.45it/s]\n[NeMo W 2026-01-25 16:03:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.20it/s]\n[NeMo W 2026-01-25 16:03:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.15it/s]\n[NeMo W 2026-01-25 16:03:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.45it/s]\n[NeMo W 2026-01-25 16:03:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.88it/s]\n[NeMo W 2026-01-25 16:03:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 16:03:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.70it/s]\n[NeMo W 2026-01-25 16:03:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.76it/s]\n[NeMo W 2026-01-25 16:03:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.22it/s]\n[NeMo W 2026-01-25 16:03:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.07it/s]\n[NeMo W 2026-01-25 16:03:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.83it/s]\n[NeMo W 2026-01-25 16:03:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.39it/s]\n[NeMo W 2026-01-25 16:03:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.25it/s]\n[NeMo W 2026-01-25 16:03:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.85it/s]\n[NeMo W 2026-01-25 16:03:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.82it/s]\n[NeMo W 2026-01-25 16:03:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.92it/s]\n[NeMo W 2026-01-25 16:03:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.93it/s]\n[NeMo W 2026-01-25 16:03:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.66it/s]\n[NeMo W 2026-01-25 16:03:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.35it/s]\n[NeMo W 2026-01-25 16:03:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.29it/s]\n[NeMo W 2026-01-25 16:03:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.77it/s]\n[NeMo W 2026-01-25 16:03:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.35it/s]\n[NeMo W 2026-01-25 16:03:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.68it/s]\n[NeMo W 2026-01-25 16:03:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.88it/s]\n[NeMo W 2026-01-25 16:03:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.38it/s]\n[NeMo W 2026-01-25 16:03:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.50it/s]\n[NeMo W 2026-01-25 16:03:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.50it/s]\n[NeMo W 2026-01-25 16:03:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.46it/s]\n[NeMo W 2026-01-25 16:03:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.91it/s]\n[NeMo W 2026-01-25 16:03:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:01,  1.08s/it]\n[NeMo W 2026-01-25 16:03:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.09it/s]\n[NeMo W 2026-01-25 16:03:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.00it/s]\n[NeMo W 2026-01-25 16:03:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.92it/s]\n[NeMo W 2026-01-25 16:03:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.87it/s]\n[NeMo W 2026-01-25 16:03:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.30it/s]\n[NeMo W 2026-01-25 16:03:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.95it/s]\n[NeMo W 2026-01-25 16:03:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.82it/s]\n[NeMo W 2026-01-25 16:03:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.07it/s]\n[NeMo W 2026-01-25 16:03:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.86it/s]\n[NeMo W 2026-01-25 16:03:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00, 10.82it/s]\n[NeMo W 2026-01-25 16:03:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.50it/s]\n[NeMo W 2026-01-25 16:03:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.39it/s]\n[NeMo W 2026-01-25 16:03:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 16:03:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.90it/s]\n[NeMo W 2026-01-25 16:03:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.86it/s]\n[NeMo W 2026-01-25 16:03:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.04it/s]\n[NeMo W 2026-01-25 16:03:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.82it/s]\n[NeMo W 2026-01-25 16:03:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.88it/s]\n[NeMo W 2026-01-25 16:03:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:03:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.12it/s]\n[NeMo W 2026-01-25 16:04:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.80it/s]\n[NeMo W 2026-01-25 16:04:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.49it/s]\n[NeMo W 2026-01-25 16:04:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.35it/s]\n[NeMo W 2026-01-25 16:04:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.49it/s]\n[NeMo W 2026-01-25 16:04:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.94it/s]\n[NeMo W 2026-01-25 16:04:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.58it/s]\n[NeMo W 2026-01-25 16:04:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.29it/s]\n[NeMo W 2026-01-25 16:04:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.07it/s]\n[NeMo W 2026-01-25 16:04:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.68it/s]\n[NeMo W 2026-01-25 16:04:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.97it/s]\n[NeMo W 2026-01-25 16:04:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.11it/s]\n[NeMo W 2026-01-25 16:04:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.78it/s]\n[NeMo W 2026-01-25 16:04:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.61it/s]\n[NeMo W 2026-01-25 16:04:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  8.05it/s]\n[NeMo W 2026-01-25 16:04:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.40it/s]\n[NeMo W 2026-01-25 16:04:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.12it/s]\n[NeMo W 2026-01-25 16:04:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.07it/s]\n[NeMo W 2026-01-25 16:04:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.30it/s]\n[NeMo W 2026-01-25 16:04:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.71it/s]\n[NeMo W 2026-01-25 16:04:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.41it/s]\n[NeMo W 2026-01-25 16:04:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.82it/s]\n[NeMo W 2026-01-25 16:04:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  7.93it/s]\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.95it/s]\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.48it/s]\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.51it/s]\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.59it/s]\n[NeMo W 2026-01-25 16:04:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.74it/s]\n[NeMo W 2026-01-25 16:04:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.07it/s]\n[NeMo W 2026-01-25 16:04:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.10it/s]\n[NeMo W 2026-01-25 16:04:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.62it/s]\n[NeMo W 2026-01-25 16:04:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.38it/s]\n[NeMo W 2026-01-25 16:04:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.74it/s]\n[NeMo W 2026-01-25 16:04:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 16:04:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.85it/s]\n[NeMo W 2026-01-25 16:04:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.51it/s]\n[NeMo W 2026-01-25 16:04:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.51it/s]\n[NeMo W 2026-01-25 16:04:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.81it/s]\n[NeMo W 2026-01-25 16:04:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  1.52it/s]\n[NeMo W 2026-01-25 16:04:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.68it/s]\n[NeMo W 2026-01-25 16:04:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  6.66it/s]\n[NeMo W 2026-01-25 16:04:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  5.34it/s]\n[NeMo W 2026-01-25 16:04:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.60it/s]\n[NeMo W 2026-01-25 16:04:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.07it/s]\n[NeMo W 2026-01-25 16:04:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.66it/s]\n[NeMo W 2026-01-25 16:04:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  2.19it/s]\n[NeMo W 2026-01-25 16:04:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.78it/s]\n[NeMo W 2026-01-25 16:04:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  4.01it/s]\n[NeMo W 2026-01-25 16:04:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: enable_chunking,trim_silence\n[NeMo W 2026-01-25 16:04:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\nTranscribing: 1it [00:00,  3.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n================ RESULTS ================\n                  model       wer  semantic_similarity  avg_rtf  robust_wer  load_time  avg_inference_time  total_inference_time model_size                       notes\n           Whisper Base 17.607860             0.918159 0.050612   17.773238   0.866333            0.336692            336.691619     ~140MB            Fastest baseline\nDistil-Whisper Large v3 11.386741             0.940653 0.109905   10.496619   1.492785            0.613819            613.818621     ~756MB               Best tradeoff\n       Whisper Large v3 10.564716             0.940918 0.462857    9.917798   2.868547            2.824130           2824.129506     ~1.5GB            Highest accuracy\n         wav2vec2 Large  1.809427             0.977953 0.009773    1.955348   1.312349            0.066454             66.454145     ~1.2GB                   SSL model\n Canary 1B Flash (NeMo) 14.689430             0.959799 0.052756   14.616470  17.585274            0.350369            350.368545     ~1.2GB Official NVIDIA NeMo loader\n\nSaved â†’ stt_benchmark_results_extended.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}